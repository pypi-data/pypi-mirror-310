# coding: utf-8

"""
    Antimatter Public API

    Interact with the Antimatter Cloud API

    The version of the OpenAPI document: 2.0.13
    Contact: support@antimatter.io
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from typing_extensions import Annotated
from antimatter_api.models.read_context_rule_facts_inner import ReadContextRuleFactsInner
from antimatter_api.models.read_context_rule_match_expressions_inner import ReadContextRuleMatchExpressionsInner
from typing import Optional, Set
from typing_extensions import Self

class ReadContextConfigRule(BaseModel):
    """
    Information about what must be done to data when it is read from a capsule 
    """ # noqa: E501
    id: Annotated[str, Field(strict=True)] = Field(description="An identifier for a rule")
    match_expressions: Optional[List[ReadContextRuleMatchExpressionsInner]] = Field(default=None, description="A list of expressions referencing the domainIdentity, readParameters and capsule/span tags. Each expression will be ANDed together, and ANDed with the factAssertions to determine if this rule activates. ", alias="matchExpressions")
    action: StrictStr
    token_scope: Optional[StrictStr] = Field(default=None, description="if the action is Tokenize, what scope to use for the token ", alias="tokenScope")
    token_format: Optional[StrictStr] = Field(default=None, description="if the action is Tokenize, what format should the token take. Explicit is of the form tk-xxxxxx and synthetic returns something that looks like the original data type (e.g. John Smith for a name) but is in fact a token ", alias="tokenFormat")
    facts: Optional[List[ReadContextRuleFactsInner]] = Field(default=None, description="assert the existence or nonexistence of facts that reference the domainIdentity, tags and readParameters. These assertions will be ANDed together, and ANDed with the matchExpressions ")
    priority: Annotated[int, Field(le=1000000, strict=True, ge=0)] = Field(description="This rule's priority. Lower priority numbers rules are evaluated first")
    imported: StrictBool = Field(description="This rule has been merged into a read context from another domain. Note that rules inside a read context that is entirely imported will not bear the imported flag. Only rules that  have been mapped into a domain's own read context will bear the imported flag ")
    source_domain_id: Optional[Annotated[str, Field(strict=True)]] = Field(default=None, description="A globally unique identifier for a domain", alias="sourceDomainID")
    source_domain_name: Optional[StrictStr] = Field(default=None, alias="sourceDomainName")
    __properties: ClassVar[List[str]] = ["id", "matchExpressions", "action", "tokenScope", "tokenFormat", "facts", "priority", "imported", "sourceDomainID", "sourceDomainName"]

    @field_validator('id')
    def id_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if not re.match(r"^rl-[a-z0-9]{16}$", value):
            raise ValueError(r"must validate the regular expression /^rl-[a-z0-9]{16}$/")
        return value

    @field_validator('action')
    def action_validate_enum(cls, value):
        """Validates the enum"""
        if value not in set(['DenyCapsule', 'DenyRecord', 'Redact', 'Tokenize', 'Allow']):
            raise ValueError("must be one of enum values ('DenyCapsule', 'DenyRecord', 'Redact', 'Tokenize', 'Allow')")
        return value

    @field_validator('token_scope')
    def token_scope_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['unique', 'capsule', 'domain']):
            raise ValueError("must be one of enum values ('unique', 'capsule', 'domain')")
        return value

    @field_validator('token_format')
    def token_format_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['explicit', 'synthetic']):
            raise ValueError("must be one of enum values ('explicit', 'synthetic')")
        return value

    @field_validator('source_domain_id')
    def source_domain_id_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r"^dm-[1-9A-HJ-NP-Za-km-z]{11}$", value):
            raise ValueError(r"must validate the regular expression /^dm-[1-9A-HJ-NP-Za-km-z]{11}$/")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of ReadContextConfigRule from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in match_expressions (list)
        _items = []
        if self.match_expressions:
            for _item_match_expressions in self.match_expressions:
                if _item_match_expressions:
                    _items.append(_item_match_expressions.to_dict())
            _dict['matchExpressions'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in facts (list)
        _items = []
        if self.facts:
            for _item_facts in self.facts:
                if _item_facts:
                    _items.append(_item_facts.to_dict())
            _dict['facts'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of ReadContextConfigRule from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "id": obj.get("id"),
            "matchExpressions": [ReadContextRuleMatchExpressionsInner.from_dict(_item) for _item in obj["matchExpressions"]] if obj.get("matchExpressions") is not None else None,
            "action": obj.get("action"),
            "tokenScope": obj.get("tokenScope"),
            "tokenFormat": obj.get("tokenFormat"),
            "facts": [ReadContextRuleFactsInner.from_dict(_item) for _item in obj["facts"]] if obj.get("facts") is not None else None,
            "priority": obj.get("priority"),
            "imported": obj.get("imported"),
            "sourceDomainID": obj.get("sourceDomainID"),
            "sourceDomainName": obj.get("sourceDomainName")
        })
        return _obj


