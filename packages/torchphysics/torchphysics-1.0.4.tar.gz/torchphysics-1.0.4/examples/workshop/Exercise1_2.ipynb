{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Sheet 1\n",
    "\n",
    "#### 1.2 Data driven function approximation \n",
    "We want to train a neural network that approximates the function \n",
    "\\begin{align}\n",
    "    u(t; D) &= \\frac{1}{D} \\left(\\ln{\\left( \\frac{1+e^{-2\\sqrt{Dg}t}}{2} \\right)} - \\sqrt{Dg} t \\right) + H\n",
    "\\end{align}\n",
    "In this example we consider values of $D \\in [0.01, 1.0]$, $t \\in [0, 3.0]$ and fix $g=9.81$, $H=50.0$.\n",
    "\n",
    "If a GPU is available (for example in Google Colab) we recomend to enable it beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Here all parameters are defined:\n",
    "t_min, t_max = 0.0, 3.0\n",
    "D_min, D_max = 0.01, 1.0\n",
    "g, H = 9.81, 50.0\n",
    "\n",
    "# dataset size for training and testing (the batch size is the product of both) \n",
    "N_D_train, N_t_train = 500, 50\n",
    "N_D_test, N_t_test = 25, 50\n",
    "\n",
    "train_iterations = 5000\n",
    "learning_rate = 1.e-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Creating the Dataset\n",
    "For the training of the neural network, we first need to create a fitting dataset. Create four tensors `input_training, output_training, input_testing, output_testing`. With shapes and data:\n",
    "```\n",
    "input_training.shape = [N_D_train, N_t_train, 2], output_training.shape = [N_D_train, N_t_train, 1] \n",
    "```\n",
    "```\n",
    "input_training[i, k] = (D_i, t_k), output_training[i, k] = u(t_k; D_i)\n",
    "```\n",
    "Similar for the testing case. Here we want to sample $D$ randomly in our given interval (`torch.rand`) and use an equidistant grid for $t$ (`torch.linspace`). For the implementation of $u$, the functions `torch.exp, torch.sqrt` and `torch.log` are helpful. \n",
    "\n",
    "**Hint**: For inserting the $D$ values into the input-tensor, the methods `repeat_interleave` (repeats elements of a tensor) with the argument `N_t_train` and `reshape` (change the shape of a tensor) with the arguments `N_D_train, N_t_train` may be usefull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Create and fill the tensors\n",
    "input_training  = ...\n",
    "output_training = ...\n",
    "input_testing   = ...\n",
    "output_testing  = ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Defining the Neural Network\n",
    "In the next step, we define our neural network, which approximates the function. For this, we can utilize \n",
    "PyTorch pre-implemented building blocks:\n",
    "\n",
    " - `torch.nn.Linear`: one single fully connected layer. Constructed with the number of inputs and outputs\n",
    " features. \n",
    " - `torch.nn.ReLU` and `torch.nn.Tanh`: possible activation functions.\n",
    " - `torch.nn.Sequential`: sequentially evaluates the building blocks to create larger and more complex neural networks. Example: `torch.nn.Sequential(torch.nn.Linear(10, 15), torch.nn.Linear(15, 5), torch.nn.ReLU())`\n",
    "\n",
    "Build a network that has 2 input neurons for the values of $t, D$ and 1 output neuron for $u$, two hidden layers of size 20 and  Tanh-activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: implement the neural network\n",
    "model = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Writing the Training Loop\n",
    "The last step is to create the training loop, where the neural network learns from the data.\n",
    "The desired loss function and the optimizer we want to use are already pre-defined. Your task is to implement the missing steps inside the loop (e.g. evaluation of the model, computing the loss, and doing the optimization). \n",
    "The example implementation on the [Pytorch page](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-optim) is helpful for this task.\n",
    "\n",
    "Once you have finished the implementation, run all the cells and start the training. You can also run the below cell multiple times to further tune the neural network. At the end of the notebook, you can check the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Move data to GPU\n",
    "model.to(\"cuda\")\n",
    "input_training = input_training.to(\"cuda\")\n",
    "output_training = output_training.to(\"cuda\")\n",
    "\n",
    "### For the loss, we take the mean squared error and Adam for optimization.\n",
    "loss_fn = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "### Training loop\n",
    "for t in range(train_iterations):\n",
    "    ### TODO: Model evaluation, loss computation and optimization\n",
    "    \n",
    "    model_out = ...\n",
    "\n",
    "    loss = ...\n",
    "\n",
    "    # optimization step ....\n",
    "\n",
    "    ### Show current loss every 250 iterations:\n",
    "    if t == 0 or t % 250 == 249:\n",
    "        print(\"Loss at iteration %i / %i is %f\" %(t, train_iterations, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here, a check of the accruarcy of the model with the training set and a plot of \n",
    "### the solution for a given data set is implemented:\n",
    "data_index_for_plot = 0\n",
    "\n",
    "### First compute error:\n",
    "model.to(\"cpu\")\n",
    "model_out = model(input_testing)\n",
    "error = torch.abs(model_out - output_testing)\n",
    "print(\"Relative error on the test data is:\", torch.max(error) / torch.max(output_testing))\n",
    "\n",
    "### Plot solution\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Showing D value:\", input_testing[data_index_for_plot, 0, 0].item())\n",
    "plt.figure(0, figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(input_testing[data_index_for_plot, :, 1], model_out[data_index_for_plot].detach())\n",
    "plt.title(\"Neural Network\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.grid()\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(input_testing[data_index_for_plot, :, 1], output_testing[data_index_for_plot])\n",
    "plt.title(\"Reference Solution\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.grid()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(input_testing[data_index_for_plot, :, 1], error[data_index_for_plot].detach())\n",
    "plt.title(\"Absolute Difference\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.grid()\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bosch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
