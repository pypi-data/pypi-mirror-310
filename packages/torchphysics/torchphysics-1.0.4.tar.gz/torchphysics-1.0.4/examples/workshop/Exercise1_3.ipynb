{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Sheet 1\n",
    "\n",
    "#### 1.3 Physics-informed function approximation \n",
    "Previously we used a dataset to learn solutions of the ODE:\n",
    "\\begin{align*}\n",
    "    \\partial_t^2 u(t) &= D(\\partial_t u(t))^2 - g \\\\\n",
    "    u(0) &= H \\\\\n",
    "    \\partial_t u(0) &= 0\n",
    "\\end{align*}\n",
    "Now, we assume that the solution is not analytically known. Therefore we have to utilize the above differential equation in the training, which leads us to physics-informed neural networks.\n",
    "\n",
    "For simplification of the implementation, we start with a fixed value for $D=0.02$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Here all parameters are defined:\n",
    "t_min, t_max = 0.0, 3.0\n",
    "D = 0.02\n",
    "g, H = 9.81, 50.0\n",
    "\n",
    "# number of time points \n",
    "N_t = 50\n",
    "\n",
    "train_iterations = 5000\n",
    "learning_rate = 1.e-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the physics-informed training, the derivatives of the neural network have to be comupted. For this, we can use `torch.autograd.grad` (automatic differentiation). With `torch.autograd.grad` not only the gradients of neural networks can be computed but also the derivatives of general tensor operations. Here a small example for $t^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative with autograd gives:\n",
      "tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20.],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Analytical derivative is:\n",
      "tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20.],\n",
      "       grad_fn=<MulBackward0>)\n",
      "They are in agreement!\n"
     ]
    }
   ],
   "source": [
    "# Create some data points\n",
    "t = torch.linspace(0, 10, 11, requires_grad=True) # we need to set requires_grad=True, or else\n",
    "                                                  # PyTorch will not be able to compute derivatives\n",
    "u = t**2 # compute the square of the values\n",
    "# Next up, we have to take the sum over all our values to compute the derivative. This has to do \n",
    "# with the implementation in PyTorch and we just have to remember to it.\n",
    "# (the reason why is yet not so important)\n",
    "u_sum = sum(u)\n",
    "# Now we can call torch.autograd.grad:\n",
    "u_t = torch.autograd.grad(u_sum, t, create_graph=True) # create_graph=True has to be set, so one can \n",
    "                                                       # later compute derivatives of higher order\n",
    "# Autograd generally returns a tuple with multiple values, here we only need the first one:\n",
    "print(\"Derivative with autograd gives:\")\n",
    "print(u_t[0])\n",
    "print(\"Analytical derivative is:\")\n",
    "print(2*t)\n",
    "print(\"They are in agreement!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Working with `torch.autograd.grad`\n",
    "Verify, with the help of `torch.autograd.grad`, that the previously given function\n",
    "\\begin{align*}\n",
    "    u(t; D) &= \\frac{1}{D} \\left(\\ln{\\left( \\frac{1+e^{-2\\sqrt{Dg}t}}{2} \\right)} - \\sqrt{Dg} t \\right) + H\n",
    "\\end{align*}\n",
    "really solves the above ODE (e.g. numerically compute the derivatives and insert them into the ODE).\n",
    "\n",
    "**Hint** : `torch.sqrt` only works for tensor-objects, you habe to either transform $D, g$ into tensors or import the `math` package for the root computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: verify ODE solution with torch.autograd.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the time grid, a tensor for the initial time point and the neural network are given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(t_min, t_max, N_t, requires_grad=True).reshape(N_t, 1)\n",
    "t_zero = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 20), torch.nn.Tanh(), \n",
    "    torch.nn.Linear(20, 20), torch.nn.Tanh(), \n",
    "    torch.nn.Linear(20, 1)\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Implementing the physics-informed Loss\n",
    "Use `torch.autograd.grad` to complete the training loop, by implementing the loss for the differential equation and both initial conditions.\n",
    "\n",
    "Each condition needs it own loss function, which are already prepared at the top of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "t = t.to(\"cuda\")\n",
    "t_zero = t_zero.to(\"cuda\")\n",
    "\n",
    "### For the loss, we take the mean squared error and Adam for optimization.\n",
    "loss_fn_ode = torch.nn.MSELoss() \n",
    "loss_fn_initial_position = torch.nn.MSELoss() \n",
    "loss_fn_initial_speed = torch.nn.MSELoss() \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "### Training loop\n",
    "for k in range(train_iterations):\n",
    "    ### TODO: implement loss computation of all equations\n",
    "    ### Loss for the differential equation: u_tt = D*(u_t)^2 - g\n",
    "    u = ...\n",
    "    u_t = ...\n",
    "\n",
    "    loss_ode = loss_fn_ode(..., ...)\n",
    "\n",
    "    ### Loss for initial condition: u(0) = H\n",
    "    u_zero = ...\n",
    "    loss_initial_position = loss_fn_initial_position(..., ...)\n",
    "\n",
    "    ### Loss for the initial velocity: u_t(0) = 0\n",
    "    \n",
    "    loss_initial_speed = loss_fn_initial_speed(..., ...)\n",
    "\n",
    "    ### Add all loss terms\n",
    "    total_loss = loss_ode + loss_initial_position + loss_initial_speed\n",
    "\n",
    "    ### Show current loss every 250 iterations:\n",
    "    if k % 250 == 0 or k == train_iterations - 1:\n",
    "        print(\"Loss at iteration %i / %i is %f\" %(k, train_iterations, total_loss.item()))\n",
    "\n",
    "    ### Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "### Here, a check of the accruarcy of the model:\n",
    "t_plot = torch.linspace(t_min, t_max, 1000).reshape(-1, 1)\n",
    "\n",
    "### First compute error:\n",
    "model.to(\"cpu\")\n",
    "model_out = model(t_plot)\n",
    "\n",
    "sqrt_term = math.sqrt(D * g)\n",
    "real_out = H - 1/D * (torch.log((1 + torch.exp(-2*sqrt_term*t_plot))/2.0) + sqrt_term*t_plot)\n",
    "\n",
    "### Plot solution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(0, figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(t_plot, model_out.detach())\n",
    "plt.title(\"Neural Network\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.grid()\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(t_plot, real_out)\n",
    "plt.title(\"Reference Solution\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.grid()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(t_plot, torch.abs(real_out - model_out).detach())\n",
    "plt.title(\"Absolute Difference\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.grid()\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bosch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
