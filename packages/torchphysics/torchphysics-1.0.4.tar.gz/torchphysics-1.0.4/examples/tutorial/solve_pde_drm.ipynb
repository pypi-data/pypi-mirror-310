{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving a simple PDE with the DeepRitz-approach\n",
    "===========================================\n",
    "Here we give a beginner-friendly introduction to TorchPhysics, going over all the basic concepts and steps. For a more in-depth explanation, we recommend the [tutorial page](https://boschresearch.github.io/torchphysics/tutorial/tutorial_start.html).\n",
    "We introduce the library with the aim to solve the following PDE:\n",
    "\n",
    "\\begin{align*}\n",
    "-\\Delta u &= 4.25\\pi^2 u \\text{ in } \\Omega = [0, 1] \\times [0, 1] \\\\\n",
    "u &= \\sin(\\tfrac{\\pi}{2} x_1)\\cos(2\\pi x_2) \\text{ on } \\partial \\Omega\n",
    "\\end{align*}\n",
    "\n",
    "For comparison, the analytic solution is $u(x_1, x_2) = \\sin(\\tfrac{\\pi}{2} x_1)\\cos(2\\pi x_2)$.\n",
    "\n",
    "Generally, the first step is to define all appearing variables and giving them a *name*. In TorchPhysics all input variables are considered as variables that have to be named, but also the solution functions. \n",
    "From a mathematical point of view we essentially define to what ``space`` these variables *belong* (for example $x \\in \\mathbb{R}^2$). From a more applied point, we just set the name and dimension of our input and output values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torchphysics as tp \n",
    "X = tp.spaces.R2('x') # input is 2D and named x\n",
    "U = tp.spaces.R1('u') # output is 1D and named u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the domain, in our case a simple square. There are a lot of different domains provided in TorchPhysics (even logical operations and time dependencies are possible), these will be introduced further later in the tutorial and can be found under the section ``domains``. \n",
    "\n",
    "Usually a domain gets as an input the space it belongs to (here our 'x') and some different parameters that depend on the constructed object. For a parallelogram, for example the origin and two corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = tp.domains.Parallelogram(X, [0, 0], [1, 0], [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our neural network, that we want to train. There are different architectures pre implemented, but since we build upon PyTorch one can easily define custom networks and use them.\n",
    "\n",
    "Generally, the DeepRitz approach uses a ResNet structure with cubic ReLU functions as an activation. In TorchPhysics all classes that handle the networks are collected under the ``models`` section. And the standard DeepRitz net is called ``DeepRitzNet``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tp.models.DeepRitzNet(input_space=X, output_space=U, width=50, depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the definition of the training conditions. Here we transform our PDE into some residuals that we minimize in the training. From an implementation point, we stay close to the original (mathematical) PDE and to the standard DeepRitz approach.\n",
    "\n",
    "Here we have two different conditions that the network should fulfill, the differential\n",
    "equation itself and the boundary condition. Here, we start with the boundary condition:\n",
    "\n",
    "  - For this, one has to first define a Python-function, that describes our trainings condition. \n",
    "    As an input, one can pick all variables and networks that appear in the problem and were defined \n",
    "    previously, via the ``spaces``. The output should describe how well the considered condition is fulfilled. \n",
    "    In our example, we just compute the expected boundary values and return the difference to the current network output. Here, ``u`` will already be the network evaluated at the points ``x`` (a batch of coordinates). Internally, this will then be transformed automatically to the integral-loss (which can also be customized, if needed) of the DeepRitz-method.\n",
    "  - We also need to tell on which points this condition should be fulfilled. For this TorchPhysics provides\n",
    "    the ``samplers`` section. Where different sampling strategies are implemented. \n",
    "    For the boundary condition we only need points at the boundary, in TorchPhysics all domains have the property ``.boundary`` that returns the boundary as a new domain-object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# Frist the function that defines the residual:\n",
    "def bound_residual(u, x):\n",
    "    bound_values = torch.sin(np.pi/2*x[:, :1]) * torch.cos(2*np.pi*x[:, 1:])\n",
    "    return (u - bound_values)**2\n",
    "\n",
    "# the point sampler:\n",
    "# here we use grid points, but any other sampler could also be used\n",
    "bound_sampler = tp.samplers.RandomUniformSampler(square.boundary, n_points=50000)\n",
    "#bound_sampler = bound_sampler.make_static() # grid always the same, therfore static for one single computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all this is defined, we have to combine the residual and sampler in a ``condition``. These condition handle internally the training process. Under the hood, they have the following simplified behavior (while training):\n",
    "\n",
    " 1) Sample points with the given sampler\n",
    " 2) Evaluate model at these points\n",
    " 3) Plug points and model output into the given residual \n",
    " 4) Compute corresponding loss term\n",
    " 5) Pass loss to the optimizer\n",
    "\n",
    "In TorchPhysics many different condition types are pre implemented (for including data, integral conditions, etc.). Here we use the DeepRitz approach, which corresponds to a ``DeepRitzCondition``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_cond = tp.conditions.DeepRitzCondition(module=model, sampler=bound_sampler, \n",
    "                                             integrand_fn=bound_residual, weight=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same holds for the differential equation term. Here also different operators are implemented, that help to compute the derivatives of the neural network. They can be found under the ``utils`` section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again a function that defines the residual:\n",
    "def energy_residual(u, x):\n",
    "    grad_term = torch.sum(tp.utils.grad(u, x)**2, dim=1, keepdim=True)\n",
    "    out = 0.5*(grad_term - 4.25*np.pi**2 * u*torch.sin(np.pi/2*x[:, :1]) * torch.cos(2*np.pi*x[:, 1:]))\n",
    "    return out\n",
    "\n",
    "# the point sampler, for the trainig points:\n",
    "pde_sampler = tp.samplers.RandomUniformSampler(square, n_points=120000) \n",
    "pde_sampler = pde_sampler.make_static(resample_interval=30)\n",
    "# wrap everything together in the condition\n",
    "pde_cond = tp.conditions.DeepRitzCondition(module=model, sampler=pde_sampler, \n",
    "                                           integrand_fn=energy_residual, weight=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation of our PDE into a TorchPhysics problem is finished. So we can start the\n",
    "training.\n",
    "\n",
    "The last step before the training is the creation of a ``Solver``. This is an object that inherits from\n",
    "the Pytorch Lightning *LightningModule*. It handles the training and validation loops and takes care of the\n",
    "data loading for GPUs or CPUs. It gets the following inputs:\n",
    "\n",
    "- train_conditions: A list of all train conditions\n",
    "- val_conditions: A list of all validation conditions (optional)\n",
    "- optimizer_setting: With this, one can specify what optimizers, learning, and learning-schedulers \n",
    "  should be used. For this, there exists the class *OptimizerSetting* that handles all these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we start with Adam:\n",
    "optim = tp.OptimizerSetting(optimizer_class=torch.optim.Adam, lr=0.0001)\n",
    "\n",
    "solver = tp.solver.Solver(train_conditions=[bound_cond, pde_cond], optimizer_setting=optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the trainer, for this we use Pytorch Lightning. Almost all functionalities of\n",
    "Pytorch Lightning can be applied in the trainings process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "## maybe selcet the GPU to use:\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "trainer = pl.Trainer(devices=1, \n",
    "                    #  accelerator=\"gpu\", # what to use to solve problem and how many devices\n",
    "                     num_sanity_val_steps=0,\n",
    "                     benchmark=True,\n",
    "                     max_steps=2500, # number of training steps\n",
    "                     logger=False, \n",
    "                     enable_checkpointing=False)\n",
    "                     \n",
    "trainer.fit(solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to have a look on our solution, we can use the plot-methods of TorchPhysics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tp.OptimizerSetting(optimizer_class=torch.optim.LBFGS, lr=0.05, \n",
    "                            optimizer_args={'max_iter': 2, 'history_size': 100})\n",
    "\n",
    "bound_cond.sampler = bound_cond.sampler.make_static() \n",
    "pde_cond.sampler = pde_cond.sampler.make_static()# LBFGS can not work with varing points!\n",
    "solver = tp.solver.Solver(train_conditions=[bound_cond, pde_cond], optimizer_setting=optim)\n",
    "\n",
    "trainer = pl.Trainer(devices=1, \n",
    "                    #  accelerator=\"gpu\", # what to use to solve problem and how many devices\n",
    "                     num_sanity_val_steps=0,\n",
    "                     benchmark=True,\n",
    "                     max_steps=1000, # number of training steps\n",
    "                     logger=False, \n",
    "                     enable_checkpointing=False)\n",
    "                     \n",
    "trainer.fit(solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampler = tp.samplers.PlotSampler(plot_domain=square, n_points=640, device='cuda')\n",
    "fig = tp.utils.plot(model, lambda u : u, plot_sampler, plot_type='contour_surface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the error, since we know the exact solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fn(u, x):\n",
    "    exact = torch.sin(np.pi/2*x[:, :1])*torch.cos(2*np.pi*x[:, 1:])\n",
    "    return torch.abs(u - exact)\n",
    "fig = tp.utils.plot(model, plot_fn, plot_sampler, plot_type='contour_surface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you saw the basics on solbing a PDE in TorchPhysics, additional examples can \n",
    "be found under the [example-folder](https://github.com/boschresearch/torchphysics/tree/main/examples).\n",
    "\n",
    "More in-depth information can be found in the [tutorial](https://torchphysics.readthedocs.io/en/latest/tutorial/tutorial_start.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb770cb910411e790a99fd848f827dc995ac53be5098d939fbaa56bcec3c9277"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
