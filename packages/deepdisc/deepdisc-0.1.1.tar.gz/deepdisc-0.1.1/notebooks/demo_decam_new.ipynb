{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e155eb64-94e2-4129-a798-cfa149839eb2",
   "metadata": {},
   "source": [
    "## Demo with DECam data\n",
    "\n",
    "This walkthrough uses the Burke et al. 2019 dataset, but superceeds the old Matterport Mask R-CNN implementation.\n",
    "\n",
    "#### A few notes:\n",
    "\n",
    "The data can be obtained following the links in the [old repository](https://github.com/burke86/astro_rcnn). The dataset directories should be re-named \"test\", \"train\", and \"val\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import SimpleTrainer\n",
    "from detectron2.engine import HookBase\n",
    "from detectron2.structures import BoxMode\n",
    "import detectron2.solver as solver\n",
    "import detectron2.modeling as modeler\n",
    "import detectron2.data as data\n",
    "import detectron2.data.transforms as T\n",
    "import detectron2.checkpoint as checkpointer\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "import weakref\n",
    "import copy\n",
    "import torch\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "\n",
    "from astropy.io import fits\n",
    "import glob\n",
    "\n",
    "import deepdisc.astrodet.astrodet as toolkit\n",
    "from deepdisc.astrodet import detectron as detectron_addons\n",
    "\n",
    "from deepdisc.data_format.file_io import DDLoader\n",
    "from deepdisc.data_format.annotation_functions.annotate_decam import annotate_decam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ac5bf-0892-49c7-9a49-674770cabf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some versions so we know what works\n",
    "print(torch.__version__)\n",
    "print(detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5be142-b975-41e2-9ad0-eddc20d7f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettify the plotting\n",
    "from deepdisc.astrodet.astrodet import set_mpl_style\n",
    "\n",
    "set_mpl_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b27c12-e635-43d1-82ee-c85115bacfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: change the dirpath here\n",
    "dirpath = \"/home/shared/hsc/decam/decam_data/\"  # Path to dataset\n",
    "output_dir = \"/home/shared/hsc/decam/models/\"\n",
    "\n",
    "dataset_names = [\"train\", \"test\", \"val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-controversy",
   "metadata": {},
   "source": [
    "### Register Astro R-CNN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261cf5a",
   "metadata": {},
   "source": [
    "For detectron2 to read the data, it must be in a dictionary format. The flexible `DDLoader` class can be used to load data from a generic directory \n",
    "structure into a user-defined output structure of metadata. Below, we iterate over each dataset and initialize a `DDLoader` instance. The `DDLoader.generate_filedict` function is used to read the directory structure and return a dictionary of file names. We can then use the `DDLoader.generate_dataset_dict` function, which\n",
    "passes a user-defined annotation function along to the files, using the \n",
    "generated dictionary of filenames.\n",
    "\n",
    "In this case, we have a pre-made annotation function for DECAM data,\n",
    "`annotate_decam`, which is passed along.\n",
    "\n",
    "However, this step can take a few minutes, and so we recommend only running it once and saving the dictionary data as a json file that can be \n",
    "read in at the beginning of your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(dataset_names):\n",
    "    filenames_dir = os.path.join(dirpath, d)\n",
    "    \n",
    "    # Generate the dictionary of filenames\n",
    "    decam_loader = DDLoader().generate_filedict(filenames_dir, \n",
    "                                                ['g', 'r', 'z'], \n",
    "                                                'img*.fits', \n",
    "                                                'masks.fits', \n",
    "                                                subdirs=True, \n",
    "                                                filt_loc=-6, \n",
    "                                                n_samples=20)\n",
    "    \n",
    "    # Register the dataset generator functions\n",
    "    DatasetCatalog.register(\"astro_\" + d, lambda: decam_loader.generate_dataset_dict(annotate_decam, filters=False).get_dataset())\n",
    "    MetadataCatalog.get(\"astro_\" + d).set(thing_classes=[\"star\", \"galaxy\"], things_colors=[\"blue\", \"gray\"])\n",
    "astro_metadata = MetadataCatalog.get(\"astro_train\")\n",
    "dataset_dicts = {}\n",
    "\n",
    "# for i, d in enumerate(dataset_names):\n",
    "for i, d in enumerate(dataset_names):\n",
    "    print(f\"Loading {d}\")\n",
    "    dataset_dicts[d] = decam_loader.generate_dataset_dict(annotate_decam, filters=False).get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code snippet for unregistering if you want to change something\n",
    "\n",
    "\"\"\"\n",
    "if \"astro_train\" in DatasetCatalog.list():\n",
    "    print('removing astro_train')\n",
    "    DatasetCatalog.remove(\"astro_train\")\n",
    "    \n",
    "    \n",
    "if \"astro_test\" in DatasetCatalog.list():\n",
    "    print('removing astro_test')\n",
    "    DatasetCatalog.remove(\"astro_test\")\n",
    "    \n",
    "if \"astro_val\" in DatasetCatalog.list():\n",
    "    print('removing astro_val')\n",
    "    DatasetCatalog.remove(\"astro_val\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e33ea",
   "metadata": {},
   "source": [
    "Run the following hidden cells if your data is already saved in dictionary format. You will need to change file paths. If you already registered the data, you will need to run the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cdbef0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a DDLoader class, which will just be used to load existing files\n",
    "json_loader = DDLoader()\n",
    "\n",
    "trainfile = os.path.join(dirpath, \"train.json\")\n",
    "testfile = os.path.join(dirpath, \"test.json\")\n",
    "valfile = os.path.join(dirpath, \"val.json\")\n",
    "\n",
    "DatasetCatalog.register(\"astro_train\", lambda: json_loader.load_coco_json_file(trainfile).get_dataset())\n",
    "MetadataCatalog.get(\"astro_train\").set(thing_classes=[\"star\", \"galaxy\"])\n",
    "astrotrain_metadata = MetadataCatalog.get(\"astro_train\")  # astro_test dataset needs to exist\n",
    "\n",
    "DatasetCatalog.register(\"astro_test\", lambda: json_loader.load_coco_json_file(testfile).get_dataset())\n",
    "MetadataCatalog.get(\"astro_test\").set(thing_classes=[\"star\", \"galaxy\"])\n",
    "astrotest_metadata = MetadataCatalog.get(\"astro_test\")\n",
    "# astro_test dataset needs to exist\n",
    "\n",
    "\n",
    "DatasetCatalog.register(\"astro_val\", lambda: json_loader.load_coco_json_file(valfile).get_dataset())\n",
    "MetadataCatalog.get(\"astro_val\").set(thing_classes=[\"star\", \"galaxy\"])\n",
    "# astroval_metadata = MetadataCatalog.get(\"astro_val\") # astro_test dataset needs to exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32d7f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dicts = {}\n",
    "json_loader = DDLoader()\n",
    "\n",
    "for i, d in enumerate(dataset_names):\n",
    "    print(f\"Loading {d}\")\n",
    "    filenames_dir = os.path.join(dirpath, d)\n",
    "    dataset_dicts[d] = json_loader.load_coco_json_file(filenames_dir + \".json\").get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7744f-6bf6-4b0b-bf98-75fac74eccd7",
   "metadata": {},
   "source": [
    "### Visualize ground truth examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample = 3  # Number of example images to plot\n",
    "fig, axs = plt.subplots(1, nsample, figsize=(5 * nsample, 5))\n",
    "\n",
    "for i, d in enumerate(random.sample(dataset_dicts[\"test\"], nsample)):\n",
    "    # Use the Lupton scaling for better visualization\n",
    "    img = toolkit.read_image_decam(d[\"file_name\"], normalize=\"astrolupton\", stretch=100, Q=10)\n",
    "\n",
    "    visualizer = Visualizer(img, metadata=astro_metadata)\n",
    "    # Plot the figures\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    axs[i].imshow(out.get_image())\n",
    "    axs[i].axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-bulgaria",
   "metadata": {},
   "source": [
    "### Data Augmentations\n",
    "\n",
    "Below, we create the function train_mapper, which takes one of the metadata dictionaries, reads in the corresponding image, and applies custom\n",
    "augmentations.  It will output a new dictionary that will be fed into the model. You can see an example of the augmentations working below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52418e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters.flip as flip\n",
    "import imgaug.augmenters.blur as blur\n",
    "\n",
    "\n",
    "def hflip(image):\n",
    "    return flip.fliplr(image)\n",
    "\n",
    "\n",
    "def gaussblur(image):\n",
    "    aug = iaa.GaussianBlur(sigma=(0.0, np.random.random_sample() * 4 + 2))\n",
    "    return aug.augment_image(image)\n",
    "\n",
    "\n",
    "def addelementwise(image):\n",
    "    aug = iaa.AddElementwise((-40, 40))\n",
    "    return aug.augment_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c135e",
   "metadata": {},
   "source": [
    "The KRandomAugmentationList class will take a list of augmentations and and randomly apply k of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cbe31-ec5a-4660-947a-8f5538ef84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mapper(dataset_dict):\n",
    "\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "\n",
    "    image = toolkit.read_image_decam(dataset_dict[\"file_name\"], normalize=\"astrolupton\", stretch=100, Q=15)\n",
    "\n",
    "    augs = detectron_addons.KRandomAugmentationList(\n",
    "        [\n",
    "            # my custom augs\n",
    "            T.RandomRotation([-90, 90, 180], sample_style=\"choice\"),\n",
    "            T.RandomFlip(prob=0.5),\n",
    "            T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n",
    "            detectron_addons.CustomAug(gaussblur, prob=1.0),\n",
    "            #detectron_addons.CustomAug(addelementwise, prob=1.0),\n",
    "        ],\n",
    "        k=-1,\n",
    "    )\n",
    "\n",
    "    # Data Augmentation\n",
    "    auginput = T.AugInput(image)\n",
    "    # Transformations to model shapes\n",
    "    transform = augs(auginput)\n",
    "    image = torch.from_numpy(auginput.image.copy().transpose(2, 0, 1))\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(annotation, [transform], image.shape[1:])\n",
    "        for annotation in dataset_dict.pop(\"annotations\")\n",
    "    ]\n",
    "    return {\n",
    "        # create the format that the model expects\n",
    "        \"image\": image,\n",
    "        \"image_shaped\": auginput.image,\n",
    "        \"height\": 512,\n",
    "        \"width\": 512,\n",
    "        \"image_id\": dataset_dict[\"image_id\"],\n",
    "        \"instances\": utils.annotations_to_instances(annos, image.shape[1:]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ca6af",
   "metadata": {},
   "source": [
    "Plot the original and augmented image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10 * 2, 10))\n",
    "\n",
    "d = random.sample(dataset_dicts[\"train\"], 1)[0]\n",
    "\n",
    "\n",
    "img = toolkit.read_image_decam(d[\"file_name\"], normalize=\"astrolupton\", stretch=100, Q=15)\n",
    "visualizer = Visualizer(img, metadata=astro_metadata, scale=1)\n",
    "# Get the ground truth boxes\n",
    "gt_boxes = np.array([a[\"bbox\"] for a in d[\"annotations\"]])\n",
    "# Convert to the mode visualizer expects\n",
    "gt_boxes = BoxMode.convert(gt_boxes, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
    "out = visualizer.overlay_instances(boxes=gt_boxes)\n",
    "axs[0].imshow(out.get_image())\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "aug_d = train_mapper(d)\n",
    "img_aug = aug_d[\"image_shaped\"]\n",
    "visualizer = Visualizer(img_aug, metadata=astro_metadata, scale=1)\n",
    "# Convert to the mode visualizer expects\n",
    "out = visualizer.overlay_instances(boxes=aug_d[\"instances\"].gt_boxes)\n",
    "axs[1].imshow(out.get_image())\n",
    "axs[1].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1962b2-c35b-4cee-85cb-4f9f0719d8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90715c5-ffba-46a3-9440-a80704050dad",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac35e7-2916-4b62-a68b-d4c4c04b82ac",
   "metadata": {},
   "source": [
    "We prepare for training by intializing a config object.  The we can take the intial weights from the pre-trained models in the model zoo.\n",
    "This setup is for demo purposes, so it does not follow a full training schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-replication",
   "metadata": {},
   "source": [
    "### Prepare For Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13232a",
   "metadata": {},
   "source": [
    "We prepare for training by intializing a config object and setting hyperparameters.  The we can take the intial weights from the pre-trained models in the model zoo.  For a full list of available config options, check https://detectron2.readthedocs.io/en/latest/modules/config.html\n",
    "\n",
    "This setup is for demo purposes, so it does not follow the full training schedule we use for the paper.  You can check the train_decam.py script for the final training configurations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe96e01-b86b-4d10-90d4-e15d26c3f9fc",
   "metadata": {},
   "source": [
    "The model used here is not as good at transfer learning to astronomical images, so the results may not appear very good for the relatively short amount of iterations used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef54d6-b3df-45f0-8ba0-29a86a74d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgfile = '../configs/solo/demo_r50_hsc.py'          # The config file which contains information about the model \n",
    "cfg = LazyConfig.load(cfgfile)                       # Load in the config\n",
    "model = return_lazy_model(cfg,freeze=False)          # Build the model from the config specifications\n",
    "cfg.optimizer.params.model = model                   # Set up the training optimizer\n",
    "optimizer = return_optimizer(cfg)\n",
    "\n",
    "\n",
    "\n",
    "loader = loaders.return_train_loader(cfg, train_mapper)      # Set up the loader, which formats the data to be fed into the model\n",
    "\n",
    "schedulerHook = return_schedulerhook(optimizer)      # Create a \"hook\" which will set up the scheduler to control learning rates\n",
    "saveHook = return_savehook(\"model_temp\")             # Create a \"hook\" which will save the model\n",
    "hookList = [saveHook, schedulerHook]                 \n",
    "\n",
    "cfg.train.init_checkpoint = \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"   #Initialize the model weights from a pre-trained model\n",
    "\n",
    "cfg.OUTPUT_DIR ='./'                                 #Set the output directory\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e96c6b-55c6-431d-9702-b1bf39c3c475",
   "metadata": {},
   "source": [
    "Now we can train the model!  We set up a trainer and tell it how often to output and when to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer = toolkit.NewAstroTrainer(model, loader, optimizer, cfg)\n",
    "#trainer.register_hooks(hookList)\n",
    "trainer = return_lazy_trainer(model, loader, optimizer, cfg, hookList)\n",
    "trainer.set_period(50)  # print loss every 10 iterations\n",
    "trainer.train(0, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10028e3b-0f2b-48e4-b88a-35b7da19642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack if you get SSL certificate error\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    # ignore ShapelyDeprecationWarning from fvcore\n",
    "    # This comes from the cropping\n",
    "    from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning)\n",
    "\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-guatemala",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepdisc.training.trainers import (\n",
    "    return_evallosshook,\n",
    "    return_lazy_trainer,\n",
    "    return_optimizer,\n",
    "    return_savehook,\n",
    "    return_schedulerhook,\n",
    ")\n",
    "from detectron2.config import LazyConfig\n",
    "\n",
    "\n",
    "from deepdisc.model.models import return_lazy_model\n",
    "\n",
    "import deepdisc.model.loaders as loaders\n",
    "from deepdisc.data_format.image_readers import HSCImageReader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-illinois",
   "metadata": {},
   "source": [
    "### Plot The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "ax.plot(trainer.lossList, label=r\"$L_{\\rm{tot}}$\")\n",
    "# ax.plot(losses, label=r'$L_{\\rm{tot}}$')\n",
    "\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel(\"training epoch\", fontsize=20)\n",
    "ax.set_ylabel(\"loss\", fontsize=20)\n",
    "ax.set_ylim(0,10)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-inspection",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757e219",
   "metadata": {},
   "source": [
    "Inference should use the config with parameters that are used in training\n",
    "cfg now already contains everything we've set previously. We changed it a little bit for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgfile = '../configs/solo/demo_r50_hsc.py'\n",
    "cfg = LazyConfig.load(cfgfile)\n",
    "cfg.OUTPUT_DIR = './'\n",
    "cfg.train.init_checkpoint = os.path.join(cfg.OUTPUT_DIR, \"model_temp.pth\")\n",
    "\n",
    "#change these to play with the detection sensitivity\n",
    "model.roi_heads.box_predictor.test_score_thresh = 0.3\n",
    "#model.roi_heads.box_predictor.test_nms_thresh = 0.5\n",
    "\n",
    "predictor = toolkit.AstroPredictor(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539340d0-b808-429d-8153-4f51ddc20b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbb424-204c-418a-888e-3e67568b6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "nsample = 3\n",
    "fig, axs = plt.subplots(1, nsample, figsize=(5 * nsample, 5))\n",
    "\n",
    "for i, d in enumerate(random.sample(dataset_dicts[\"test\"], nsample)):\n",
    "    img = toolkit.read_image_decam(d[\"file_name\"], normalize=\"astrolupton\", stretch=100, Q=15)\n",
    "    outputs = predictor(\n",
    "        img\n",
    "    )  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "\n",
    "    print(\"total instances:\", len(d[\"annotations\"]))\n",
    "    print(\"detected instances:\", len(outputs[\"instances\"].pred_boxes))\n",
    "    print(\"\")\n",
    "\n",
    "    v = Visualizer(\n",
    "        img,\n",
    "        metadata=astro_metadata,\n",
    "        scale=1,\n",
    "        instance_mode=ColorMode.SEGMENTATION,  # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    axs[i].imshow(out.get_image())\n",
    "    axs[i].axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ecc9c-93ec-4413-afbc-4e0d1286fcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "textile-stereo",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c425fe-e848-4337-8942-672f800f8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mapper(dataset_dict):\n",
    "\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "\n",
    "    image = toolkit.read_image_decam(dataset_dict[\"file_name\"], normalize=\"astrolupton\", stretch=100, Q=10)\n",
    "    augs = T.AugmentationList([])\n",
    "    # Data Augmentation\n",
    "    auginput = T.AugInput(image)\n",
    "    # Transformations to model shapes\n",
    "    transform = augs(auginput)\n",
    "    image = torch.from_numpy(auginput.image.copy().transpose(2, 0, 1))\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(annotation, [transform], image.shape[1:])\n",
    "        for annotation in dataset_dict.pop(\"annotations\")\n",
    "    ]\n",
    "    return {\n",
    "        # create the format that the model expects\n",
    "        \"image\": image,\n",
    "        \"image_shaped\": auginput.image,\n",
    "        \"height\": 512,\n",
    "        \"width\": 512,\n",
    "        \"image_id\": dataset_dict[\"image_id\"],\n",
    "        \"instances\": utils.annotations_to_instances(annos, image.shape[1:]),\n",
    "        \"annotations\": annos,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-manor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "# NOTE: New version has max_dets_per_image argument in default COCOEvaluator\n",
    "evaluator = toolkit.COCOEvaluatorRecall(\"astro_val\", use_fast_impl=True, output_dir=cfg.OUTPUT_DIR)\n",
    "\n",
    "test_loader = build_detection_test_loader(dataset_dicts[\"val\"], mapper=test_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2fb53-2ed7-4d7b-a34e-0437e8b94ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = inference_on_dataset(predictor.model, test_loader, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"bbox\"][\"AP-star\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_type = \"bbox\"  # Which type of precision/recall to use? 'segm', or 'bbox'\n",
    "cls_names = [\"star\", \"galaxy\"]\n",
    "\n",
    "results_per_category = results[ap_type][\"results_per_category\"]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "ious = np.linspace(0.50, 0.95, 10)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(ious)))\n",
    "\n",
    "# Plot precision recall\n",
    "for j, precision_class in enumerate(results_per_category):\n",
    "    precision_shape = np.shape(precision_class)\n",
    "    for i in range(precision_shape[0]):\n",
    "        # precision has dims (iou, recall, cls, area range, max dets)\n",
    "        # area range index 0: all area ranges\n",
    "        # max dets index -1: typically 100 per image\n",
    "        p_dat = precision_class[i, :, j, 0, -1]\n",
    "        # Hide vanishing precisions\n",
    "        mask = p_dat > 0\n",
    "        # Only keep first occurance of 0 value in array\n",
    "        mask[np.cumsum(~mask) == 1] = True\n",
    "        p = p_dat[mask]\n",
    "        # Recall points\n",
    "        r = np.linspace(0, 1, len(p))  # Recall is always defined from 0 to 1 for these plots, I think\n",
    "        dr = np.diff(np.linspace(0, 1, len(p_dat)))[0]  # i think\n",
    "        # Plot\n",
    "        iou = np.around(ious[i], 2)\n",
    "        AP = 100 * np.sum(p * dr)\n",
    "        axs[j].plot(\n",
    "            r, p, label=r\"${\\rm{AP}}_{%.2f} = %.1f$\" % (iou, AP), color=colors[i], lw=2\n",
    "        )  # use a viridis color scheme\n",
    "        axs[j].set_xlabel(\"Recall\", fontsize=20)\n",
    "        axs[j].set_ylabel(\"Precision\", fontsize=20)\n",
    "        axs[j].set_xlim(0, 1.1)\n",
    "        axs[j].set_ylim(0, 1.1)\n",
    "        axs[j].legend(fontsize=10, title=f\"{cls_names[j]}\", bbox_to_anchor=(1.35, 1.0))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28b02e-e656-4541-98fc-dc8ed0ed8c84",
   "metadata": {},
   "source": [
    "This demo is just to show how to set up the training.  We encourage you to add object classes, try different contrast scalings, and train for longer!  \n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "  You can also look at the content of the output below  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0502acc9-a9bb-44e8-8494-16e77977c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = predictor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e2590-4f27-4d60-9bc3-91936587c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['instances'].get_fields().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945026d-263b-4483-949e-a89c15898705",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs['instances'].scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9a729-185f-48d6-bbc0-e817e013e639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ddrailnv]",
   "language": "python",
   "name": "conda-env-.conda-ddrailnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
