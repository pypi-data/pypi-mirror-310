# -*- coding: utf-8 -*-
"""Weighted.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YQ95wI-wzXmHgpq41Y0fTDgia0tEZq7o
"""

#test.r from the main version to python Post-hoc Optimization Code.r from the cleaner version
#weighted ARIMA used in the following parts????

#observe kivabe calculate korse?
#MAIN MLAMA Visualization here

import pandas as pd
import numpy as np
from scipy.optimize import minimize


# Cost function to minimize
def cost_function(weights, predictions, observed):
    #weighted_predictions = np.dot(predictions, weights)
    #weighted_predictions = [a*b for a,b in zip(predictions, weights)]
    weighted_predictions = predictions*weights
    r = np.sum((observed - weighted_predictions) ** 2)#squared error
    # If sum of weights is not 1, apply a penalty (optional)
    #if np.sum(weights) != 1:
     #   r += 1000000000000000
    #print(r)
    return r

# Function to find optimal weights for each unique combination of Delay and Length
def find_optimal_weights(data, predictions, delay_list, observed_column_name):
    optimal_weights = {}

    # Iterate over unique combinations of Delay and Length delay, prediction_pength
    #for combo in data[['Delay', 'Length']].drop_duplicates().itertuples(index=False):
    for length in predictions:
      for delay in delay_list:
          #print(delay, length)
          #delay, length = combo
          combo_data = data[(data['Delay'] == delay) & (data['Length'] == length)].groupby('Wave').mean()
          #print(combo_data)

          # Extract the predictions and observed values
          predicted_value = combo_data[['XGBoost', 'RF', 'ARIMA']].values
          observed = combo_data[observed_column_name].values

          # Initial weights inverse squared
          x = 1 / combo_data.filter(like='diff_').pow(2)
          initial_weights = (x.div(x.sum(axis=1), axis=0)).mean().values

          # Optimizing weights
          opt_res = minimize(cost_function, initial_weights, args=(predicted_value, observed),
                            method='L-BFGS-B', bounds=[(0, 1)]*3)

          # Storing the results
          optimal_weights[f"{delay} {length}"] = opt_res.x

    return optimal_weights


optimization_filename = 'all'+'_model_predictions_'+filename
#optimization_filename= "post-hoc optimization.xlsx"
observed_column_name = 'Observed'
#observed_column_name = 'Observe'
# Load the data
visualization_data = pd.read_csv(inFoldername_pre+optimization_filename)
#visualization_data = pd.read_excel(inFoldername_pre+optimization_filename)

visualization_data = visualization_data.drop(columns=['week'])
#visualization_data = pd.read_csv(optimization_filename,index_col=0)



# Calculate the differences
visualization_data['diff_XGB'] = visualization_data['XGBoost'] - visualization_data[observed_column_name]
visualization_data['diff_RF'] = visualization_data['RF'] - visualization_data[observed_column_name]
visualization_data['diff_ARIMA'] = visualization_data['ARIMA'] - visualization_data[observed_column_name]
# Executing the function
optimal_weights = find_optimal_weights(visualization_data, predictions, delay_list, observed_column_name)


# Standardizing the optimal weights
stan_optimal_weights = {k: v/np.sum(v) for k, v in optimal_weights.items()}


# Combine results into a DataFrame
combined_df = pd.DataFrame()
for scenario_name, weights in stan_optimal_weights.items():
    delay, length = map(float, scenario_name.split())
    #print(delay, length)
    #temp_df = pd.DataFrame(weights).T
    #temp_df['Delay'] = delay
    #temp_df['Length'] = length

    #print(delay, prediction_length)
    # Create a temporary DataFrame with the weights and scenario details
    temp_df = pd.DataFrame([weights], columns=['weight_XGB', 'weight_RF', 'weight_ARIMA'])
    temp_df['Delay'] = delay
    temp_df['Length'] = length

    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)

# Merging with original data
joined_df = pd.merge(combined_df, visualization_data, on=['Delay', 'Length'])

# Calculating weighted_y
#
joined_df['weighted_y'] = (joined_df['XGBoost'] * joined_df['weight_XGB']) + \
                          (joined_df['RF'] * joined_df['weight_RF']) + \
                          (joined_df['ARIMA'] * joined_df['weight_ARIMA'])

# Calculate MAPE
joined_df['XGBoost_MAPE'] = abs(joined_df['XGBoost'] - joined_df[observed_column_name]) / joined_df[observed_column_name]
joined_df['RF_MAPE'] = abs(joined_df['RF'] - joined_df[observed_column_name]) / joined_df[observed_column_name]
joined_df['ARIMA_MAPE'] = abs(joined_df['ARIMA'] - joined_df[observed_column_name]) / joined_df[observed_column_name]
joined_df['Weighted_MAPE'] = abs(joined_df['weighted_y'] - joined_df[observed_column_name]) / joined_df[observed_column_name]

# Grouping by Delay, Wave, and Length, and summarizing MAPE
theD = joined_df.groupby(['Delay', 'Wave', 'Length']).agg(
    XGBoost_MAPE=('XGBoost_MAPE', 'mean'),
    RF_MAPE=('RF_MAPE', 'mean'),
    ARIMA_MAPE=('ARIMA_MAPE', 'mean'),
    Weighted_MAPE=('Weighted_MAPE', 'mean')
).reset_index()

# Convert percentages
theD[['XGBoost_MAPE', 'RF_MAPE', 'ARIMA_MAPE', 'Weighted_MAPE']] *= 100

#print(theD)

#import pandas as pd
#import seaborn as sns
#import matplotlib.pyplot as plt
#Visulization Code.R from the cleaner version MAPE.R from the main version, supposed to run only p1, and p2?? p-combined??test3 for plot
# Load Data
#theDf = pd.read_csv(inFoldername_pre+"data3.csv")
#theDf.columns = ["Wave", "Adaptability", "Prediction Length", "XGBoost_MAPE", "RF_MAPE", "ARIMA_MAPE", "Weighted_MAPE"]

#Adaptability ==delay?
theD.columns = ["Adaptability", "Wave", "Prediction Length", "XGBoost_MAPE", "RF_MAPE", "ARIMA_MAPE", "Weighted_MAPE"]

# Rename Columns
theD['Random_Forest_MAPE'] = theD['RF_MAPE']
theD['ARIMABoost_MAPE'] = theD['Weighted_MAPE']

# Convert from wide to long format
theD_long = pd.melt(theD, id_vars=["Wave", "Adaptability", "Prediction Length"],
                    value_vars=["XGBoost_MAPE", "Random_Forest_MAPE", "ARIMA_MAPE", "ARIMABoost_MAPE"],
                    var_name="Algorithm", value_name="MAPE")

# Remove '_MAPE' suffix from Algorithm names
theD_long['Algorithm'] = theD_long['Algorithm'].str.replace('_MAPE', '')


# Group data and calculate mean MAPE
theDD = theD_long.groupby(['Algorithm', 'Adaptability']).agg({'MAPE': 'mean'}).reset_index()



# P1 Plot 1: Adaptability vs MAPE
plt.figure(figsize=(10, 6))
sns.lineplot(data=theDD, x='Adaptability', y='MAPE', hue='Algorithm', linewidth=2.0)
plt.ylabel('SMAPE')
plt.xlabel('Adaptability/Delay is x axis. Delayed train+test, how many additional weeks are added for training,  \n so testing starts from later point in time, test size depends on prediction_length,\n for all the waves and prediction length averaged to calculate MAPE. \nNumber of Weeks of Data Since the Local Min/Max')
plt.title('a. Responsiveness')
plt.show()

# Group data by Prediction Length
theDD = theD_long.groupby(['Algorithm', 'Prediction Length']).agg({'MAPE': 'mean'}).reset_index()
#print(theDD)
# P2 Plot 2: Prediction Length vs MAPE
plt.figure(figsize=(10, 6))
sns.lineplot(data=theDD, x='Prediction Length', y='MAPE', hue='Algorithm', linewidth=2.0)
plt.ylabel('SMAPE')
plt.xlabel('Number of Weeks in Forecast. averaged for all delay, wave')
plt.title('b. Prediction Length')
plt.show()