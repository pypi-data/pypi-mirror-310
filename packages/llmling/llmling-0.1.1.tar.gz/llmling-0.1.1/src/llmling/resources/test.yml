# Context processors definitions
context_processors:
  python_cleaner:
    type: function
    import_path: llmling.testing.processors.uppercase_text
  sanitize:
    type: function
    import_path: llmling.testing.processors.multiply

  add_metadata:
    type: template
    template: |
      # Generated at: {{ now() }}
      # Source: {{ source }}
      # Version: {{ version }}

      {{ content }}

# Global tools definitions
tools:
  analyze_code:
    import_path: "llmling.tools.code.analyze_complexity"
    description: "Analyze Python code complexity metrics"
  analyze_ast:
    import_path: "llmling.testing.tools.analyze_ast"
    description: "Analyze Python code structure"
  format_code:
    import_path: "llmling.tools.code.format_code"
    name: "format_python"

# LLM provider configurations
llm_providers:
  gpt4-turbo:
    name: "GPT-4 Turbo"
    model: openai/gpt-4-1106-preview
    temperature: 0.8
    max_tokens: 4096
    top_p: 0.95
    # Can now use either format:
    tools:
      - analyze_code # Simple string reference
      - analyze_ast
      - format_code
    # Or with settings:
    # tools:
    #   analyze_code: {}
    #   analyze_ast: {max_lines: 1000}
    #   format_code: {style: "black"}

  claude2:
    name: "Claude 2"
    model: anthropic/claude-2
    temperature: 0.7
    max_tokens: 8192

  local-llama:
    name: "Local Llama"
    model: openai/gpt-3.5-turbo
    temperature: 0.7
    max_tokens: 2048

# Provider groups
provider_groups:
  code_review:
    - gpt4-turbo
    - claude2

  draft_content:
    - local-llama

  fallback_chain:
    - gpt4-turbo
    - claude2
    - local-llama

# Context definitions
contexts:
  python_guidelines:
    type: path
    path: "https://example.com/python-guidelines.md"
    description: "Python coding standards and best practices"
    processors:
      - name: sanitize
        keyword_args: { remove_emails: true }
      - name: add_metadata
        keyword_args:
          source: "company guidelines"
          version: "1.2.3"

  my_utils:
    type: source
    import_path: "my_project.utils"
    description: "Utility module source code"
    recursive: true
    processors:
      - name: python_cleaner

  single_module:
    type: source
    import_path: "my_project.models.user"
    description: "User model implementation"
    recursive: false
    include_tests: false

  system_info:
    type: callable
    import_path: "my_project.utils.system_diagnostics.get_info"
    description: "Current system information"
    keyword_args:
      include_memory: true
      include_disk: true

  code_review_template:
    type: path
    path: "./templates/code_review.txt"
    description: "Template for code review prompts"
    processors:
      - name: python_cleaner

  system_prompt:
    type: text
    content: |
      You are a test assistant. Your task is to generate exactly 100 words
      of test content. The content should be a simple story about a dog.

      Requirements:
      1. Exactly 100 words
      2. Simple narrative structure
      3. Consistent tone
      4. No complex vocabulary
      5. No dialogue

      Begin your response immediately with the story, without any preamble
      or meta-commentary.
    description: "Test prompt for consistent output"

  git_diff:
    type: cli
    command: "git diff HEAD~1"
    description: "Current git changes"
    shell: true
    processors:
      - name: python_cleaner

# Context groups
context_groups:
  code_review_basic:
    - system_prompt
    - code_review_template

  code_review_advanced:
    - system_prompt
    - code_review_template
    - python_guidelines
    - git_diff

# Task templates
task_templates:
  quick_review:
    provider: local-llama
    context: system_prompt
    settings:
      temperature: 0.7
      max_tokens: 2048

  detailed_review:
    provider: code_review
    context: code_review_advanced
    settings:
      temperature: 0.5
      max_tokens: 4096

  generate_draft:
    provider: draft_content
    context: system_prompt
    settings:
      temperature: 0.9
      max_tokens: 2048
