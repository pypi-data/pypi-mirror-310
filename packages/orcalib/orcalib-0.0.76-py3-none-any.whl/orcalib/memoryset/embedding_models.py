from __future__ import annotations

import logging
import os
import random
from dataclasses import dataclass
from enum import Enum
from typing import Literal, cast

import numpy as np
from sentence_transformers import SentenceTransformer
from torch import Tensor
from tqdm.auto import trange
from transformers import AutoConfig, CLIPConfig, PretrainedConfig

from orcalib.memoryset.embedding_finetuning import (
    EmbeddingTrainingArguments,
    EmbeddingTrainingArgumentsForClassification,
    EmbeddingTrainingArgumentsWithTriplets,
    finetune_for_classification,
    finetune_with_triplets,
)
from orcalib.memoryset.util import transform_data_to_dataset

from ..torch_layers import SentenceEmbeddingGenerator
from .memory_types import DatasetLike, InputType, InputTypeList


class EmbeddingFinetuningMethod(str, Enum):
    CLASSIFICATION = "classification"
    """Fine tune embeddings by adding a logistic regression head and training to predict labels"""

    TRIPLETS = "triplets"
    """Fine tune embeddings via contrastive triplet loss based on labels"""


@dataclass(frozen=True)
class _Config:
    """Schema for relevant parsed values from the AutoConfig of an embedding model"""

    name: str
    """Either the name of a HuggingFace model or a path to a local saved model"""

    max_seq_length: int
    """The maximum sequence length to use with this model"""

    embedding_dim: int
    """The dimension of the embeddings generated by this model"""

    query_prompt: str | None
    """Optional prompt prefix to use for queries with this model"""

    document_prompt: str | None
    """Optional prompt prefix to use for documents with this model"""

    transductive_context_length: int | None
    """The number of documents to use for the transductive context of contextual embedding models"""

    auto_model_compatible: bool
    """Whether this model can be used with AutoModel in addition to SentenceTransformer"""


class EmbeddingModelMeta(type):
    @property
    def CLIP_BASE(cls) -> EmbeddingModel:
        """[CLIP-L14](https://huggingface.co/sentence-transformers/clip-ViT-L-14) embedding model"""
        return EmbeddingModel("OrcaDB/clip-ViT-L-14")

    @property
    def GTE_BASE(cls) -> EmbeddingModel:
        """[Alibaba GTE-Base v1.5](https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5) embedding model"""
        return EmbeddingModel("OrcaDB/gte-base-en-v1.5")

    @property
    def CDE_SMALL(cls) -> EmbeddingModel:
        """[CDE-Small](https://huggingface.co/jxm/cde-small-v1) embedding model"""
        return EmbeddingModel("OrcaDB/cde-small-v1")


class EmbeddingModel(metaclass=EmbeddingModelMeta):
    """Embedding models for use with memorysets"""

    model_whitelist = [
        "Alibaba-NLP/gte-large-en-v1.5",
        "sentence-transformers/multi-qa-mpnet-base-dot-v1",
        "distilbert-base-uncased",
        "distilbert-base-cased",
        "bert-base-cased",
        "bert-base-uncased",
        "roberta-base",
        "roberta-large",
        "microsoft/mpnet-base",
    ]

    @staticmethod
    def load_config(name: str, trust_remote_code: bool = False) -> _Config:
        config: PretrainedConfig = AutoConfig.from_pretrained(
            name, trust_remote_code=trust_remote_code or name.startswith("OrcaDB/") or os.path.isdir(name)
        )
        embedding_dim = getattr(config, "embedding_size", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "projection_dim", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "hidden_size", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "d_model", None)
        if embedding_dim is None:
            embedding_dim = getattr(config, "n_embed", None)
        if embedding_dim is None:
            raise ValueError(f"Could not determine embedding dimension from {config}")
        max_seq_length = getattr(config, "max_seq_length", None)
        if max_seq_length is None and hasattr(config, "text_config"):
            max_seq_length = getattr(config.text_config, "max_position_embeddings", None)
        if max_seq_length is None:
            max_seq_length = getattr(config, "max_position_embeddings", None)
        if max_seq_length is None:
            max_seq_length = getattr(config, "n_positions", None)
        if max_seq_length is None:
            raise ValueError(f"Could not determine max sequence length from {config}")
        query_prompt = getattr(config, "query_prompt", None)
        document_prompt = getattr(config, "document_prompt", None)
        transductive_context_length = getattr(config, "transductive_corpus_size", None)
        return _Config(
            name=name,
            embedding_dim=embedding_dim,
            max_seq_length=max_seq_length,
            query_prompt=query_prompt,
            document_prompt=document_prompt,
            transductive_context_length=transductive_context_length,
            auto_model_compatible=(
                not isinstance(config, CLIPConfig) and (type(config).__name__ != "ContextualModelConfig")
            ),
        )

    def __init__(self, name: str, *, trust_remote_code: bool = False, max_seq_length: int | None = None):
        """
        Initialize an embedding model

        Warning:
            Only the models that are available as class properties like `EmbeddingModel.CLIP_BASE` as
            well as fine-tuned versions of them are guaranteed to work.

        Args:
            name: the name of the embedding model
            trust_remote_code: whether to trust remote model implementation code from sources other
                than HuggingFace and Orca's organizations, don't st this to true in a server environment
            max_seq_length: optional overwrite for the maximum sequence length (in tokens) that the
                model will accept, if `None` the model's default max sequence length will be used
        """
        is_orca_model = name.startswith("OrcaDB/")
        self.trust_remote_code = is_orca_model or trust_remote_code or os.path.isdir(name)
        self.config = self.load_config(name, trust_remote_code=self.trust_remote_code)
        if not is_orca_model and name not in self.model_whitelist and not os.path.isdir(name):
            logging.warning(f"Model {name} is not in the whitelist, it may not work correctly")
        self.name = name
        if max_seq_length is not None and max_seq_length > self.config.max_seq_length:
            raise ValueError(
                f"Max sequence length {max_seq_length} is greater than what the model supports: {self.config.max_seq_length}"
            )
        self._max_seq_length_overwrite = max_seq_length
        self.transductive_context: None | Tensor = None

    @property
    def uses_context(self) -> bool:
        return self.config.transductive_context_length is not None

    @property
    def embedding_dim(self) -> int:
        return self.config.embedding_dim

    @property
    def max_seq_length(self) -> int:
        return self._max_seq_length_overwrite or self.config.max_seq_length

    def __repr__(self) -> str:
        return f"EmbeddingModel({self.name}, embedding_dim={self.embedding_dim}, max_seq_length={self.max_seq_length})"

    # We want only the _embedder to be a singleton to ensure that the context is not shared
    _embedder_cache: dict[tuple[str, int | None], SentenceTransformer | SentenceEmbeddingGenerator] = {}

    @property
    def _embedder(self) -> SentenceTransformer | SentenceEmbeddingGenerator:
        cache_key = (self.name, self._max_seq_length_overwrite)
        if cache_key not in self._embedder_cache:
            if self.config.auto_model_compatible:
                self._embedder_cache[cache_key] = SentenceEmbeddingGenerator(
                    base_model=self.name,
                    frozen=True,
                    normalize=True,
                    trust_remote_code=self.trust_remote_code,
                    max_sequence_length=self._max_seq_length_overwrite,
                )
            else:
                embedder = SentenceTransformer(self.name, trust_remote_code=self.trust_remote_code)
                if self._max_seq_length_overwrite is not None:
                    embedder.max_seq_length = self._max_seq_length_overwrite
                self._embedder_cache[cache_key] = embedder
        return self._embedder_cache[cache_key]

    def update_context(self, values: list[str]) -> None:
        """
        Update the context used by contextual embedding models like [CDE](https://huggingface.co/jxm/cde-small-v1)

        Args:
            values: the values of the corpus to construct the context from
        """
        if self.config.transductive_context_length is not None and isinstance(self._embedder, SentenceTransformer):
            logging.info("Updating transductive context for embedding model")
            if len(values) > self.config.transductive_context_length:
                sampled_values = random.sample(values, k=self.config.transductive_context_length)
            else:
                sampled_values = random.choices(values, k=self.config.transductive_context_length)
            self.transductive_context = self._embedder.encode(
                sampled_values,
                prompt=self.config.document_prompt,
                convert_to_tensor=True,
                show_progress_bar=False,
            )

    def embed(
        self,
        values: InputType | InputTypeList,
        show_progress_bar: bool = False,
        batch_size: int = 32,
        value_kind: Literal["query", "document"] = "query",
    ) -> np.ndarray:
        """
        Generate embeddings for the given input

        Args:
            data: the data to encode, will be converted to a list if a scalar is given
            show_progress_bar: whether to show a progress bar
            batch_size: size of the batches to use
            value_kind: kind of values to embed, either "query" or "document" to determine potential
                prompts for the model, this is usually just used by memoryset internally
        Returns:
            matrix with embeddings of shape `len_data` x `embedding_dim`
        """
        values = [values] if not isinstance(values, list) else values
        if len(values) == 0:
            return np.empty((0,))
        # generate embeddings
        if isinstance(self._embedder, SentenceTransformer):
            return self._embedder.encode(
                values,  # type: ignore -- types are wrong, image is accepted here
                show_progress_bar=show_progress_bar,
                normalize_embeddings=True,
                batch_size=batch_size,
                prompt=self.config.document_prompt if value_kind == "document" else self.config.query_prompt,
                dataset_embeddings=self.transductive_context,
            )
        else:
            if not isinstance(values[0], str):
                raise ValueError(f"{self.name} embedding model only supports strings")
            if len(values) <= batch_size:
                return self._embedder.encode(cast(list[str], values)).cpu().numpy()
            else:
                results = []
                for i in trange(
                    0,
                    len(values),
                    batch_size,
                    disable=not show_progress_bar,
                ):
                    batch = cast(list[str], values[i : i + batch_size])
                    results.append(self._embedder.encode(batch).cpu().numpy())
                return np.vstack(results)

    def finetune(
        self,
        save_dir: str,
        train_dataset: DatasetLike,
        eval_dataset: DatasetLike | None = None,
        training_args: EmbeddingTrainingArguments | None = None,
        method: EmbeddingFinetuningMethod | str = EmbeddingFinetuningMethod.CLASSIFICATION,
    ) -> EmbeddingModel:
        """
        Finetune the embedding model on a given dataset

        Args:
            save_dir: The directory to save the finetuned model to.
            train_dataset: The data to finetune on.
            eval_dataset: The data to evaluate the finetuned model on, if this is `None` a 10%
                holdout from the training data will be used.
            training_args: The training arguments to use for the finetuning. If this is `None`
                sensible defaults will be used based on the finetuning method.
            method: The method to use for finetuning, "triplets" uses a contrastive triplet loss to
                pull embeddings together and push them apart based on labels. "classification" adds
                a logistic regression head to the model and fine-tunes it for classification.

        Returns:
            New finetuned embedding model
        """
        train_dataset = transform_data_to_dataset(train_dataset)
        if eval_dataset is not None:
            eval_dataset = transform_data_to_dataset(eval_dataset)
        else:
            split_dataset = train_dataset.train_test_split(test_size=0.1)
            train_dataset = split_dataset["train"]
            eval_dataset = split_dataset["test"]

        match method:
            case EmbeddingFinetuningMethod.CLASSIFICATION:
                training_args = training_args or EmbeddingTrainingArgumentsForClassification()
                if training_args.max_seq_length is None and self._max_seq_length_overwrite is not None:
                    # if the model has a custom max sequence length, use it for finetuning
                    training_args.max_seq_length = self._max_seq_length_overwrite
                finetune_for_classification(
                    self.name,
                    save_dir,
                    train_dataset,
                    eval_dataset,
                    training_args,
                )
            case EmbeddingFinetuningMethod.TRIPLETS:
                training_args = training_args or EmbeddingTrainingArgumentsWithTriplets()
                if training_args.max_seq_length is None and self._max_seq_length_overwrite is not None:
                    # if the model has a custom max sequence length, use it for finetuning
                    training_args.max_seq_length = self._max_seq_length_overwrite
                finetune_with_triplets(
                    self.name,
                    save_dir,
                    train_dataset,
                    eval_dataset,
                    training_args,
                )
            case _:
                raise ValueError(f"Invalid finetuning method: {method}")

        # return a new embedding model by loading it from the saved directory
        return EmbeddingModel(
            save_dir,
            trust_remote_code=self.trust_remote_code,
            max_seq_length=self._max_seq_length_overwrite,
        )
