{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51eef4c2-f75e-4330-b72f-fd5b69ed2111",
   "metadata": {},
   "source": [
    "# Submitting complex metadata\n",
    "\n",
    "So far, in the previous notebooks, we have learned how to submit and perform operations on simple samples, that is; samples that are isolated in the submission, and all its metadata is just a bunch of simple values.\n",
    "\n",
    "However, life is not as simple; sometimes, you will want to (and will have to, as seen in the notebook about sample validation) attach some extra metadata to your samples, that being how it relates to other samples (is it a replicate? is it derived from another sample that you are submitting or has been submitted?) or that extra bit of metadata that makes your fields very demure (e.g. specifying a unit, or ❤️referencing an ontology value❤️)\n",
    "\n",
    "On this notebook, we will focus on exactly that, with the following TOC:\n",
    "\n",
    "1. Before you start\n",
    "2. Submitting samples with relationships\n",
    "    1. All samples are being submitted for the first time\n",
    "    2. Some samples are already accessioned; some are not\n",
    "    3. All samples are accessioned but need to update relationships\n",
    "3. How to improve your metadata via units and ontologies\n",
    "4. Structured data\n",
    "    1. WTH is structured data\n",
    "    2. Structured data format and posting to sample\n",
    "5. Posting a external URL (reference) to a sample\n",
    "6. Validating against custom checklists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132218fe-6f91-4ea8-8136-707675c1bfe8",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "### Make sure you have biobroker >= 0.1.0\n",
    "Before we start, we're going to do the same we always do; however, a lot of features for this notebook need `biobroker>=0.1.0` (The release where pydantic was introduced!) so... let's start with that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed3731f-585f-4442-9a19-fce10c1a8967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biobroker in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.0.7)\n",
      "Requirement already satisfied: numpy~=1.23.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from biobroker) (1.23.1)\n",
      "Requirement already satisfied: openpyxl==3.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from biobroker) (3.1.2)\n",
      "Requirement already satisfied: pandas~=2.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from biobroker) (2.0.3)\n",
      "Requirement already satisfied: progressbar2~=4.4.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from biobroker) (4.4.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from biobroker) (2.31.0)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openpyxl==3.1.2->biobroker) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas~=2.0.3->biobroker) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas~=2.0.3->biobroker) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas~=2.0.3->biobroker) (2024.2)\n",
      "Requirement already satisfied: python-utils>=3.8.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from progressbar2~=4.4.2->biobroker) (3.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.31.0->biobroker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.31.0->biobroker) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.31.0->biobroker) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.31.0->biobroker) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas~=2.0.3->biobroker) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>3.10.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-utils>=3.8.1->progressbar2~=4.4.2->biobroker) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datamodel-code-generator in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: argcomplete<4.0,>=1.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datamodel-code-generator) (3.5.1)\n",
      "Requirement already satisfied: black>=19.10b0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datamodel-code-generator) (24.10.0)\n",
      "Requirement already satisfied: genson<2.0,>=1.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datamodel-code-generator) (1.3.0)\n",
      "Requirement already satisfied: inflect<6.0,>=4.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datamodel-code-generator) (5.6.2)\n",
      "Requirement already satisfied: isort<6.0,>=4.3.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datamodel-code-generator) (5.13.2)\n",
      "Requirement already satisfied: jinja2<4.0,>=2.10.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datamodel-code-generator) (3.1.4)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datamodel-code-generator) (24.1)\n",
      "Requirement already satisfied: pydantic!=2.4.0,<3.0,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel-code-generator) (2.9.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datamodel-code-generator) (6.0.2)\n",
      "Requirement already satisfied: toml<1.0.0,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datamodel-code-generator) (0.10.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from black>=19.10b0->datamodel-code-generator) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from black>=19.10b0->datamodel-code-generator) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from black>=19.10b0->datamodel-code-generator) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from black>=19.10b0->datamodel-code-generator) (4.3.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from black>=19.10b0->datamodel-code-generator) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from black>=19.10b0->datamodel-code-generator) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2<4.0,>=2.10.1->datamodel-code-generator) (2.1.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=2.4.0,<3.0,>=1.9.0->pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel-code-generator) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=2.4.0,<3.0,>=1.9.0->pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel-code-generator) (2.23.4)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel-code-generator) (2.2.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from email-validator>=2.0.0->pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel-code-generator) (2.7.0)\n",
      "Requirement already satisfied: idna>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from email-validator>=2.0.0->pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel-code-generator) (3.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install biobroker --upgrade\n",
    "%pip install datamodel-code-generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1adbceb-7841-47e5-9e91-b308d70a1b9b",
   "metadata": {},
   "source": [
    "(Please make sure you re-start the kernel if you upgraded your version!)\n",
    "\n",
    "### Set-up and create samples\n",
    "Once that's set-up, we're going to import everything and set up the basic metadata for 4 samples; the reason for setting up 4 samples this time will become apparent in the next sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19527188-a658-41d7-8cce-3aa0841f09ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 12:23:56,713 - BsdApi - INFO - Set up BSD API successfully: using base uri 'https://wwwdev.ebi.ac.uk/biosamples/samples'\n",
      "2024-10-14 12:24:04,626 - Biosample - ERROR - Metadata content has failed validation for 'Sample 1':\n",
      "\t- characteristics: Missing mandatory field 'project name'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n",
      "\t- characteristics: Missing mandatory field 'collection date'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n",
      "\t- characteristics: Missing mandatory field 'geographic location (country and/or sea)'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n",
      "\t- characteristics: Missing mandatory field 'geographic location (latitude)'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n",
      "\t- characteristics: Missing mandatory field 'geographic location (longitude)'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n",
      "\t- characteristics: Missing mandatory field 'broad-scale environmental context'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n",
      "\t- characteristics: Missing mandatory field 'local environmental context'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n",
      "\t- characteristics: Missing mandatory field 'environmental medium'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n",
      "\t- characteristics: Missing mandatory field 'elevation'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n",
      "\t- characteristics: Missing mandatory field 'depth'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n",
      "2024-10-14 12:24:08,926 - Biosample - ERROR - Metadata content has failed validation for 'Sample 1':\n",
      "\t- characteristics-->geographic location (country and/or sea)-->0-->text: Input should be 'Afghanistan', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Arctic Ocean', 'Argentina', 'Armenia', 'Aruba', 'Ashmore and Cartier Islands', 'Atlantic Ocean', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Baker Island', 'Baltic Sea', 'Bangladesh', 'Barbados', 'Bassas da India', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Borneo', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Virgin Islands', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Clipperton Island', 'Cocos Islands', 'Colombia', 'Comoros', 'Cook Islands', 'Coral Sea Islands', 'Costa Rica', \"Cote d'Ivoire\", 'Croatia', 'Cuba', 'Curacao', 'Cyprus', 'Czechia', 'Democratic Republic of the Congo', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'East Timor', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Europa Island', 'Falkland Islands (Islas Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern and Antarctic Lands', 'Gabon', 'Gambia', 'Gaza Strip', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Glorioso Islands', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Honduras', 'Hong Kong', 'Howland Island', 'Hungary', 'Iceland', 'India', 'Indian Ocean', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Jan Mayen', 'Japan', 'Jarvis Island', 'Jersey', 'Johnston Atoll', 'Jordan', 'Juan de Nova Island', 'Kazakhstan', 'Kenya', 'Kerguelen Archipelago', 'Kingman Reef', 'Kiribati', 'Kosovo', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macau', 'Macedonia', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mediterranean Sea', 'Mexico', 'Micronesia', 'Midway Islands', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Navassa Island', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'North Korea', 'North Sea', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pacific Ocean', 'Pakistan', 'Palau', 'Palmyra Atoll', 'Panama', 'Papua New Guinea', 'Paracel Islands', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn Islands', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Republic of the Congo', 'Reunion', 'Romania', 'Ross Sea', 'Russia', 'Rwanda', 'Saint Helena', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'South Korea', 'Southern Ocean', 'Spain', 'Spratly Islands', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard', 'Swaziland', 'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Tajikistan', 'Tanzania', 'Tasman Sea', 'Thailand', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tromelin Island', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'USA', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Viet Nam', 'Virgin Islands', 'Wake Island', 'Wallis and Futuna', 'West Bank', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe', 'missing: control sample', 'missing: data agreement established pre-2023', 'missing: endangered species', 'missing: human-identifiable', 'missing: lab stock', 'missing: sample group', 'missing: synthetic construct', 'missing: third party data', 'not applicable', 'not collected', 'not provided' or 'restricted access'. Provided value: 'Mushroom kingdom'\n",
      "\t- characteristics-->geographic location (latitude)-->0-->text: Input should be a valid string. Provided value: '1.2234'\n",
      "\t- characteristics-->geographic location (longitude)-->0-->text: Input should be a valid string. Provided value: '7.21'\n",
      "\t- characteristics: Missing mandatory field 'elevation'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'Homo sapiens'}], 'project name': [{'text': 'Your fake project'}], 'collection date': [{'text': '2024-09-01'}], 'geographic location (country and/or sea)': [{'text': 'Mushroom kingdom'}], 'geographic location (latitude)': [{'text': 1.2234}], 'geographic location (longitude)': [{'text': 7.21}], 'broad-scale environmental context': [{'text': 'United Kingdom weather'}], 'local environmental context': [{'text': 'Mostly rainy'}], 'environmental medium': [{'text': 'Please read my plant'}]}'\n",
      "\t- characteristics: Missing mandatory field 'depth'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'Homo sapiens'}], 'project name': [{'text': 'Your fake project'}], 'collection date': [{'text': '2024-09-01'}], 'geographic location (country and/or sea)': [{'text': 'Mushroom kingdom'}], 'geographic location (latitude)': [{'text': 1.2234}], 'geographic location (longitude)': [{'text': 7.21}], 'broad-scale environmental context': [{'text': 'United Kingdom weather'}], 'local environmental context': [{'text': 'Mostly rainy'}], 'environmental medium': [{'text': 'Please read my plant'}]}'\n",
      "2024-10-14 12:24:29,603 - BsdApi - ERROR - Found following errors in sample validation:\n",
      "\t- geographic location (latitude)/0.unit: should have required property 'unit'\n",
      "\t- geographic location (longitude)/0.unit: should have required property 'unit'\n",
      "\t- elevation/0.unit: should have required property 'unit'\n",
      "\t- depth/0.unit: should have required property 'unit')\n"
     ]
    }
   ],
   "source": [
    "## Import everything we need\n",
    "from biobroker.authenticator import WebinAuthenticator # Biosamples uses the WebinAuthenticator\n",
    "from biobroker.api import BsdApi # BioSamples Database (BSD) API\n",
    "from biobroker.metadata_entity import Biosample # The metadata entity\n",
    "from biobroker.input_processor import TsvInputProcessor # An input processor\n",
    "from biobroker.output_processor import XlsxOutputProcessor # An output processor\n",
    "import os\n",
    "\n",
    "## Generate sample\n",
    "sample_tsv = [\n",
    "    [\"name\", \"collected_at\", \"organism\", \"release\"],\n",
    "    [\"Sample 1\", \"noon\", \"homo sapiens\", \"2024-01-07\"],\n",
    "    [\"Sample 2\", \"noon\", \"homo sapiens\", \"2024-01-07\"],\n",
    "    [\"Sample 3\", \"noon\", \"homo sapiens\", \"2024-01-07\"],\n",
    "    [\"Sample 4\", \"noon\", \"homo sapiens\", \"2024-01-07\"]\n",
    "]\n",
    "\n",
    "writable_sample = \"\\n\".join([\"\\t\".join(row) for row in sample_tsv])\n",
    "with open(\"complex_sample_metadata.tsv\", \"w\") as f:\n",
    "    f.write(writable_sample)\n",
    "\n",
    "path = \"complex_sample_metadata.tsv\" # This is the file we created previously\n",
    "\n",
    "## Set up the required entities\n",
    "\n",
    "input_processor = TsvInputProcessor(input_data=path)\n",
    "\n",
    "os.environ['API_ENVIRONMENT'] = \"dev\" # There are multiple ways to set up environment variables\n",
    "\n",
    "username = \"\" # Your username goes here\n",
    "password = \"\" # Your password goes here\n",
    "authenticator = WebinAuthenticator(username=username, password=password)\n",
    "\n",
    "api = BsdApi(authenticator=authenticator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db0628c-8dbd-46cc-91f7-e5e1c92cb77b",
   "metadata": {},
   "source": [
    "## Submitting samples with relationships\n",
    "\n",
    "Depending on your metadata model, you may want to create entries in BioSamples that are related to each other; Maybe instead of repeating the important metadata from a parent sample (e.g. a cell line), you want to capture that metadata in its own entry and link the library preparations as samples derived from that one. \n",
    "\n",
    "Biosamples has got you covered! (And `biobroker` too). In BioSamples, you can link different samples that you have submitted using the attributes `derived_from`, `same_as`, `has_member` and `child_of`. You can find more information on these relationship types in https://www.ebi.ac.uk/biosamples/docs/guides/relationships. An important thing to remember about relationships is that they are **bi-directional**: That means, if you specify a type of relationship on sample A towards sample B, the inverse (or equal, in the case of `same_as`) will automatically apply on the sample\n",
    "\n",
    "`biobroker` processes these relationships by reserving the name of the relationships as keywords for the metadata entry. What that means, in practice, is: to indicate that samples are related to each on the input metadata, you just need to add a column/field with the name of the keyword.\n",
    "\n",
    "Let's go through a set of use cases so it becomes *clearer*:\n",
    "\n",
    "### All samples are being submitted for the first time\n",
    "\n",
    "Alright! So, let's say we have the four samples we have defined before. We know they are related in the following way:\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "\n",
    "A[Sample 1]\n",
    "B[Sample 2]\n",
    "C[Sample 3]\n",
    "D[Sample 4]\n",
    "\n",
    "B-->|derived_from|A\n",
    "C-->|derived_from|A\n",
    "D-->|derived_from|A\n",
    "B-->|same_as|C\n",
    "B-->|same_as|D\n",
    "C-->|same_as|D\n",
    "```\n",
    "\n",
    "Looks complicated, but all I want to represent is: All samples are derived from `Sample 1`, and the derived samples are all replicates.\n",
    "\n",
    "In this specific case, we have 4 samples that have not yet been submitted; and biosamples only accepts sample accessions as an input for the relationships. How are we going to do it?\n",
    "\n",
    "The short and sad answer is: You need to submit the samples and then update them with the relationships. \n",
    "\n",
    "The cooler answer is that I got this covered in `biobroker`! On the `BsdApi` object there is a keyword argument you can pass to the `submit` function, `process_relationships`, that when set to True, will do the trick for you so you just need to worry.\n",
    "\n",
    "But forget about all this theory and let's go into practice. The first thing we need to do is to define this relationships. You may have noticed we didn't `process` the samples before; that is because I want a clean slate for each of the examples, so we will start each example by loading the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7152e33e-13ad-45ad-b354-c47e37da376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_relationships_non_accessioned = input_processor.process(Biosample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42709c19-cb9a-4fc7-bb0f-83a1799f3da7",
   "metadata": {},
   "source": [
    "Now that we have the samples loaded in, let's add the relationships. Since they are not accessioned, we are going to add the relationships using the name of the samples and the reserved keywords we talked about before. \n",
    "\n",
    "**Please note**: This is much easier and intuitive to do on the input metadata (excel, tsv etc) directly and load it with an input processor rather doing it here, but I am not going to create a file just for these samples. Just rembember that, index-wise, sample correspond to their index + 1 (e.g. index 0 of the samples is Sample 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "000692db-2669-4c72-900f-5a28180ff940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All samples are derived from Sample 1\n",
    "samples_relationships_non_accessioned[1]['derived_from'] = \"Sample 1\"\n",
    "samples_relationships_non_accessioned[2]['derived_from'] = \"Sample 1\"\n",
    "samples_relationships_non_accessioned[3]['derived_from'] = \"Sample 1\"\n",
    "\n",
    "## Samples 2, 3 and 4 are replicates.\n",
    "samples_relationships_non_accessioned[1]['same_as'] = \"Sample 3||Sample 4\"\n",
    "samples_relationships_non_accessioned[2]['same_as'] = \"Sample 4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113f8de-fe46-45e3-9954-ace6f15adbcc",
   "metadata": {},
   "source": [
    "Now we have the metadata in order to start submitting and processing those relationships!\n",
    "\n",
    "One very important thing to notice is how I used the double pipes to indicate more than one relationship. You may need to adjust the delimiter for the different entities, depending on how you set the `Biosample`. The default delimiter for the entity is a double pipe (`||`). We will use this for multiple relationships, multiple urls, etc. It's not a perfect solution but it's a much easier to handle solution rather than starting to create columns with programmatic names and parsing that as arrays!\n",
    "\n",
    "We will now submit the samples, as usual, with a slight change: we are going to provide with the `process_relationships` keyword. What this will do, in short, is to submit the samples as is, process the relationships afterwards, and update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb79281-6444-4b9b-930a-bb478d949fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_samples = api.submit(samples_relationships_non_accessioned, process_relationships=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff7a6a-b23b-45fe-9d21-4b18c76abde3",
   "metadata": {},
   "source": [
    "Let's see what our submitted samples look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83321447-3f3e-4835-ba0a-a160f23a48a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wwwdev.ebi.ac.uk/biosamples/samples/SAMEA131399004\n"
     ]
    }
   ],
   "source": [
    "print(f\"https://wwwdev.ebi.ac.uk/biosamples/samples/{submitted_samples[0].accession}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49bdb02-2df1-4932-a03d-3912b6382d42",
   "metadata": {},
   "source": [
    "![Parent sample](samples_relationship_parent.png)\n",
    "\n",
    "As we can see... The relationships have gone through! Wooho! Now we have all of our samples linked together. This is just the example for the parent sample, but if you go to the link, you will see that the relationships are also defined for the child samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f3807-3111-48c3-81c0-ccbccc18e823",
   "metadata": {},
   "source": [
    "### Mixing accessioned and non-accessioned samples\n",
    "\n",
    "The previous example was very simple; in that case, all of our samples were not submitted, so we could define all the relationships in our input and let `BsdApi.submit()` handle the process. However, that is not going to be always the case; sometimes you will have a parent sample that you have already registered, and child samples that you produced (replicates) that need to be now registered after you've finished your experiments.\n",
    "\n",
    "For that, we are going to use the same set of samples as before, with the new relationships; just this time, we are going to submit the parent sample first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf49626-47a8-4307-a36c-ac7a1f52619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_relationships_mixed_accessioned = input_processor.process(Biosample)\n",
    "parent_sample = [samples_relationships_mixed_accessioned[0]]\n",
    "child_samples = samples_relationships_mixed_accessioned[1:]\n",
    "\n",
    "parent_sample_submitted = api.submit(parent_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5078ab7a-5dc2-4016-a912-7f22034a5583",
   "metadata": {},
   "source": [
    "Once the parent sample is submitted, we are going to indicate the relationship in the child samples by **using the accession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ad5239-a1be-40f8-b936-68f442fe343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All samples are derived from Sample 1\n",
    "child_samples[0]['derived_from'] = parent_sample_submitted[0].accession\n",
    "child_samples[1]['derived_from'] = parent_sample_submitted[0].accession\n",
    "child_samples[2]['derived_from'] = parent_sample_submitted[0].accession\n",
    "\n",
    "## Samples 2, 3 and 4 are replicates.\n",
    "child_samples[0]['same_as'] = \"Sample 3||Sample 4\"\n",
    "child_samples[1]['same_as'] = \"Sample 4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d4596-f39b-4476-9eaf-dd6f986433ad",
   "metadata": {},
   "source": [
    "Now that we have our child samples set up, let's try to submit them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db9ae60-b98b-4aa4-bc3f-0079a24bedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_samples_mixed_accessioning = api.submit(child_samples, process_relationships=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9cf1c51-88c2-4c0c-8589-979325586ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wwwdev.ebi.ac.uk/biosamples/samples/SAMEA131399017\n"
     ]
    }
   ],
   "source": [
    "print(f\"https://wwwdev.ebi.ac.uk/biosamples/samples/{submitted_samples_mixed_accessioning[0].accession}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d661b17-a25c-461b-8448-c40c598e9bd9",
   "metadata": {},
   "source": [
    "And, as we can see, the samples are submitted with the relationships attached! Isn't that neat?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdc95f-f82a-41eb-9c08-14115f3ccaaf",
   "metadata": {},
   "source": [
    "### Updating relationships\n",
    "\n",
    "Updating the relationships for already accesioned samples is pretty straight-forward; for this, you just need to retrieve the samples (You can use multiple methods for that - You can refer to notebook 4 for more information).\n",
    "\n",
    "Once you've got the samples, you just need to update the metadata to include the relationship; for this, you can either:\n",
    "* Add the value for each of the samples retrieved in your script and update them using `BsdApi.update()`\n",
    "* Output the samples to a more user-friendly format (Xlsx, tsv) modify them, load them back and update them using `BsdApi.update()`\n",
    "\n",
    "The metadata entities automatically recognise that the origin sample is accessioned, so when you add the relationship, it will be added to the proper metadata and you don't need to use the `process_relationships` function.\n",
    "\n",
    "There is a third, more complicated option, which is to specify them somewhat manually; there is a function, [`add_relationship`](https://biobroker.readthedocs.io/en/latest/biobroker.metadata_entity.html#biobroker.metadata_entity.Biosample.add_relationship) for which you can specify the relationship manually, but... I don't really think it's worth it. I made it really simple for the user!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87774037-e3da-4cf3-b64d-fdbbf74af873",
   "metadata": {},
   "source": [
    "## Adding ontology and unit tags to your values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9350f3-aaa8-45f2-9647-df246dafcaf4",
   "metadata": {},
   "source": [
    "Another cool thing that BSD has, is that for each property, you can specify a `unit` or an `ontology term`, so that the term contains a bit more extra implicit metadata without tampering with the field name (**tip**: A field name should not contain the unit of measurement! That's usually bad practice!!)\n",
    "\n",
    "Let's imagine that you have measured the depth of some soil samples that you have taken. This depth was measured in meters, and you made sure your samples got taken at 1 meter, since it's the best depth ever for these samples (Please don't hate me I know nothing about soil)\n",
    "\n",
    "You want to use the GSC MIxS soil checklist (ERC000020), and you see the `depth` field to collect this information. There is a slight problem; you don't see where to input the units. You see in other submitted samples that it is indeed specified, but there is a parenthesis encasing the unit. What is going on here?\n",
    "\n",
    "BSD has defined its own way to deal with units and ontology terms; instead of having 2 fields and linking them, they have the information defined within the field. For context, a field in BSD usually looks like this:\n",
    "```json\n",
    "...\n",
    "<field_name>: [\n",
    "    {\n",
    "        \"text\": <value_of_field>\n",
    "    }\n",
    "]\n",
    "...\n",
    "```\n",
    "\n",
    "(Now you're happy you don't have to deal with that when using this library, eh?)\n",
    "\n",
    "Adding a unit is as simple as this:\n",
    "\n",
    "```json\n",
    "...\n",
    "<field_name>: [\n",
    "    {\n",
    "        \"text\": <value_of_field>,\n",
    "        \"unit\": <unit_name>\n",
    "    }\n",
    "]\n",
    "...\n",
    "```\n",
    "\n",
    "And it's even simpler with the library. For the BioSample metadata entity, a delimiter is defined; that delimiter, aside from allowing to specify more than one relationship in the same field, it also allows you to define the unit in the metadata. Let's use the `depth` example so that it is clearer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afbc380d-396d-4c40-8ac8-39a537e5b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Sample 1', 'characteristics': {'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}], 'depth': [{'text': 1}]}, 'release': '2024-10-06T18:24:02.616045Z'}\n"
     ]
    }
   ],
   "source": [
    "samples_units = input_processor.process(Biosample)\n",
    "\n",
    "# Let's define the depth in the first sample and see what it looks like\n",
    "\n",
    "samples_units[0]['depth'] = 1\n",
    "\n",
    "print(samples_units[0].entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8785a535-eef9-41b0-96cb-b764ea2ead28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Sample 1', 'characteristics': {'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}], 'depth': [{'text': 1, 'unit': 'meter'}]}, 'release': '2024-10-06T18:24:02.616045Z'}\n"
     ]
    }
   ],
   "source": [
    "# Now, let's add the unit! This also works for the headers of the user-friendly inputs (Like excel and tsv)\n",
    "samples_units[0]['depth||unit'] = 'meter'\n",
    "\n",
    "print(samples_units[0].entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13f747-ff02-4312-9705-4e7693d30877",
   "metadata": {},
   "source": [
    "And Ta-da! we've got our sample with the `unit` attached! At this point, you can submit it to BSD and it will appear properly. As I mention in the code, you can do this in a programmatic way or you can add your units and/or ontology terms (For more information on the tags, you can read the [BSD documentation on submitting a sample](https://www.ebi.ac.uk/biosamples/docs/references/api/submit) (However, on the tags, you'll see there's only 3: 'unit', 'text' and 'ontologytTerms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a7b87-ca97-4aba-9f17-fa024493715e",
   "metadata": {},
   "source": [
    "## Structured data\n",
    "\n",
    "### What in the world is structured data\n",
    "\n",
    "There is another layer of information that you can add to a sample; when you're recording metadata for a sample, you can sometimes generate a table of information (e.g. an Antibiogram). You could attach this information to the sample in a very weird and complicated way; however, biosamples provides with the ability to provide tables with tagged information and a `title`, and they call it `structured data`. For a couple examples of structured data, you can look at this sample in production: https://www.ebi.ac.uk/biosamples/samples/SAMEA112948612 or look at the [documentation examples](https://www.ebi.ac.uk/biosamples/docs/references/api/submit#_submit_structured_data)\n",
    "\n",
    "### Structured data format\n",
    "\n",
    "Let's do a simple task of creating some very simple structured data. Let's say that, for our case, we have realised an assay on the sample, that returns a very complex 2x2 table (scary)\n",
    "\n",
    "For this, let's start by loading up the samples and getting just the first one for this task and define the table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1084ecd7-b491-4240-8f37-094abf27adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the samples\n",
    "samples_st = input_processor.process(Biosample)\n",
    "\n",
    "# Choose just the first sample and submit it\n",
    "sample_example = [samples_st[0]]\n",
    "\n",
    "sample_example_submitted = api.submit(sample_example)\n",
    "\n",
    "data_table = [[1, 34.5],\n",
    "              [2, 33.7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c6704c-251f-4d45-9ae8-fcdb712273b9",
   "metadata": {},
   "source": [
    "Now that we have all the necessary things set up, let's start structuring the table in the necessary format. In the code, this will all be one step, but take a look at the comments! I will be commenting each line that needs clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73224746-2f47-4cf3-9819-1ed4e15cd799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accession': 'SAMEA131399379', 'data': [{'webinSubmissionAccountId': 'Webin-64342', 'type': 'MYAWESOMEASSAY', 'content': [{'timepoint': {'value': 1, 'unit': 'hour'}, 'luminosity': {'value': 34.5}}, {'timepoint': {'value': 2, 'unit': 'hour'}, 'luminosity': {'value': 33.7}}]}]}\n"
     ]
    }
   ],
   "source": [
    "structured_data = {  # Please note, the structured data is submitted as a dictionary!\n",
    "    'accession': sample_example_submitted[0].accession,  # Accession must be provided with the structured data\n",
    "    'data': [ # This array is for all the different, unrelated tables you want to submit. Here, we only have one table, so the array will have lenght 1\n",
    "        {\n",
    "            'webinSubmissionAccountId': 'Webin-64342',  # For some goddam reason they need your webin account there\n",
    "            'type': 'MYAWESOMEASSAY', # This is the name of the table; usually, the name of the assay\n",
    "            # Note here: You can specify a `schema` property to validate the content of your table. Not going to do it\n",
    "            'content': [ # This is the actual data table! wooho! each of the elements of the aray is a dictionary with the values associated to a field name\n",
    "                { # We have 2 rows, so we expect 2 dictionaries\n",
    "                    'time point': { # Each field-value is defined as in the usual for BSD: 'value' is needed (instead of 'text'), and you can add tags\n",
    "                        'value': 1,\n",
    "                        'unit': 'hour'\n",
    "                    },\n",
    "                    'luminosity': {\n",
    "                        'value': 34.5\n",
    "                    }\n",
    "                },\n",
    "                { \n",
    "                    'time point': { \n",
    "                        'value': 2,\n",
    "                        'unit': 'hour'\n",
    "                    },\n",
    "                    'luminosity': {\n",
    "                        'value': 33.7\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# You could also generate the content automatically by iterating the data table. Feel free to use your preferred method!\n",
    "content = [{'timepoint': {'value': row[0], 'unit': 'hour'}, 'luminosity': {'value': row[1]}} for row in data_table] \n",
    "structured_data['data'][0]['content'] = content\n",
    "\n",
    "# Let's take a look at the structured data!\n",
    "\n",
    "print(structured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e47c9cb-744f-4dd1-bcfb-c19154ceceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"accession\" : \"SAMEA131399379\",\n",
      "  \"create\" : \"2024-10-06T19:18:11.785Z\",\n",
      "  \"update\" : \"2024-10-06T19:18:11.786Z\",\n",
      "  \"data\" : [ {\n",
      "    \"domain\" : null,\n",
      "    \"webinSubmissionAccountId\" : \"Webin-64342\",\n",
      "    \"type\" : \"MYAWESOMEASSAY\",\n",
      "    \"schema\" : null,\n",
      "    \"content\" : [ {\n",
      "      \"timepoint\" : {\n",
      "        \"value\" : \"1\",\n",
      "        \"iri\" : null\n",
      "      },\n",
      "      \"luminosity\" : {\n",
      "        \"value\" : \"34.5\",\n",
      "        \"iri\" : null\n",
      "      }\n",
      "    }, {\n",
      "      \"timepoint\" : {\n",
      "        \"value\" : \"2\",\n",
      "        \"iri\" : null\n",
      "      },\n",
      "      \"luminosity\" : {\n",
      "        \"value\" : \"33.7\",\n",
      "        \"iri\" : null\n",
      "      }\n",
      "    } ]\n",
      "  } ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = api.authenticator.post(url=f\"{api.structured_data_endpoint}/{structured_data['accession']}\", payload=structured_data)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602cb6c-abf2-4482-992a-932164ca68d3",
   "metadata": {},
   "source": [
    "(Another error I found while writing these examples - The returned response is the structured data, not the sample - will fix in 0.0.7 and use the proper function to do this)\n",
    "\n",
    "As you can see, it has returned the data created. Let's take a look at the BioSamples entry!\n",
    "\n",
    "https://wwwdev.ebi.ac.uk/biosamples/samples/SAMEA131399379\n",
    "\n",
    "![BSD Structured data](bsd_data_entry_sd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b2547-6070-4918-940c-33de950dcb92",
   "metadata": {},
   "source": [
    "## Posting an external reference (URI)\n",
    "\n",
    "The last, and probably easiest, is adding external references. In BSD, external references are used to link data that could not be hosted in BSD to the entry, to have a single `source of truth` for all the data related to a sample.\n",
    "\n",
    "One example of this are the ENA links to the Genomics/Transcriptomics data - But in any case, these links are generated automatically by BioSamples if you remember to use your sample accession in your ENA submission :)\n",
    "\n",
    "Here I will only say that, similarly to the relationships, URLs can be added with the function `add_external_reference()` on the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236f06c-96d4-45ff-aad3-47e02656d84d",
   "metadata": {},
   "source": [
    "## Validating against a custom checklist\n",
    "\n",
    "For this one, I've left the least user-friendly capability of the `biobroker` library for last: The ability to validate your own metadata via the use of the [Pydantic](https://docs.pydantic.dev/latest/) library.\n",
    "\n",
    "For a thorough explanation of what pydantic is, please consult their webpage. For us, in short, is a way to validate the metadata of our samples via the use of python classes. Sounds complicated, but don't worry, I will provide with 2 examples:\n",
    "\n",
    "- Using an already existing BioSamples checklist and transforming it to a pydantic model\n",
    "- Modifying an existing pydantic model to satisfy your validation rules\n",
    "\n",
    "Please take into account that pydantic offers a **HUGE** range of validation options: What `biobroker` does with that is, basically, offer a `validate()` method. This method:\n",
    "- Takes in a subclass of a pydantic `BaseModel`\n",
    "- Loads the data in `.entity` to that `BaseModel`. During instantiation, pydantic deals with validation.\n",
    "    - If errors: Parse error returned by Pydantic and offer it to the user.\n",
    "    - If no error: Dump the resulting JSON back to `.entity`\n",
    "\n",
    "(Why dump the resulting JSON back to `.entity` if we're just validating, you ask? Well, we're not JUST validating: pydantic also offers a bit of data formatting so, if e.g. I am expecting a string, and someone inputs a \"1\", it will transform the `1` value to `\"1\"`. Cool, right?)\n",
    "\n",
    "So, basically, in this section you will learn how to use your own checklists for validation; this may serve the purpose of pre-submission validation against a checklist, or if you are using a custom checklist (e.g. Want to use ERC000022 to validate against your checklist, but you want your consortia to specify a time point and you want to stablish rules against that specific field)\n",
    "\n",
    "### Validate your samples against an  existing checklist\n",
    "\n",
    "Let's say we want to pre-validate our samples against [ERC000022](https://www.ebi.ac.uk/ena/browser/view/ERC000022). The first step to take is to download the checklist in a JSON schema format; For that, Biosamples stores the schemas in a public schema store. The URL to the ERC000022 checlist is: https://www.ebi.ac.uk/biosamples/schema/store/registry/schemas/ERC000022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7536b28d-31f8-46ae-8ad2-81b933f84950",
   "metadata": {},
   "source": [
    "The next step is to convert this checklist to a pydantic-compatible model; for that, thankfully, there are a lot of tools. For this one, we will use the recommended tool [datamodel code generator](https://docs.pydantic.dev/latest/integrations/datamodel_code_generator/). Please make sure you follow their installation instructions before continuing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289ba761-4741-478f-91da-9dc49070d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!datamodel-codegen --url \"https://www.ebi.ac.uk/biosamples/schema/store/registry/schemas/ERC000022\" --input-file-type jsonschema --output-model-type pydantic_v2.BaseModel --output ERC000022.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689ccfdf-8130-4b3b-a238-748975fa8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ERC000022 import GscMixsSoil # We load the main class - This is the root of the schema!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65cfda3-ab01-427c-8d42-398e5898a01f",
   "metadata": {},
   "source": [
    "Alrighty! now we have loaded the model. Let's prepare our sample to be validated against this checklist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453e54a6-4604-4c90-b407-d6b1f382c6bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "EntityValidationError",
     "evalue": "Metadata content has failed validation for 'Sample 1':\n\t- characteristics: Missing mandatory field 'project name'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'collection date'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'geographic location (country and/or sea)'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'geographic location (latitude)'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'geographic location (longitude)'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'broad-scale environmental context'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'local environmental context'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'environmental medium'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'elevation'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'depth'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEntityValidationError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m samples_checklist_validation \u001b[38;5;241m=\u001b[39m input_processor\u001b[38;5;241m.\u001b[39mprocess(Biosample)\n\u001b[1;32m      2\u001b[0m sample_checklist_validation \u001b[38;5;241m=\u001b[39m samples_checklist_validation[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# Let's just take 1 sample for the example\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43msample_checklist_validation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGscMixsSoil\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# We pass the model to validate against the metadata\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/biobroker/biobroker/metadata_entity/metadata_entity.py:75\u001b[0m, in \u001b[0;36mGenericEntity.validate\u001b[0;34m(self, data_model)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity \u001b[38;5;241m=\u001b[39m data_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity)\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_unset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, by_alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pydantic_core\u001b[38;5;241m.\u001b[39mValidationError \u001b[38;5;28;01mas\u001b[39;00m pydantic_error:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntityValidationError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger, entity_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, errors\u001b[38;5;241m=\u001b[39mpydantic_error\u001b[38;5;241m.\u001b[39merrors()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mEntityValidationError\u001b[0m: Metadata content has failed validation for 'Sample 1':\n\t- characteristics: Missing mandatory field 'project name'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'collection date'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'geographic location (country and/or sea)'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'geographic location (latitude)'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'geographic location (longitude)'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'broad-scale environmental context'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'local environmental context'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'environmental medium'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'elevation'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'\n\t- characteristics: Missing mandatory field 'depth'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'homo sapiens'}]}'"
     ]
    }
   ],
   "source": [
    "samples_checklist_validation = input_processor.process(Biosample)\n",
    "sample_checklist_validation = samples_checklist_validation[0] # Let's just take 1 sample for the example\n",
    "\n",
    "sample_checklist_validation.validate(GscMixsSoil) # We pass the model to validate against the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d759910-93d3-49df-9566-48dfa60a1b44",
   "metadata": {},
   "source": [
    "Similar to what we did in notebook 2, let's correct the missing fields!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a9cec4-e5c5-4c22-99e8-2414fa80a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_checklist_validation['project name'] = \"Your fake project\"\n",
    "sample_checklist_validation['organism'] = \"Homo sapiens\"\n",
    "sample_checklist_validation['collection date'] = \"2024-09-01\"\n",
    "sample_checklist_validation['geographic location (country and/or sea)'] = \"Mushroom kingdom\"\n",
    "sample_checklist_validation['geographic location (latitude)'] = 1.2234\n",
    "sample_checklist_validation['geographic location (longitude)'] = 7.21\n",
    "sample_checklist_validation['broad-scale environmental context'] = \"United Kingdom weather\"\n",
    "sample_checklist_validation['local environmental context'] = \"Mostly rainy\"\n",
    "sample_checklist_validation['environmental medium'] = \"Please read my plant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6892fc-d668-45a1-8992-651c2169a1c9",
   "metadata": {},
   "source": [
    "And validate again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5784e2bc-775e-4332-8fa4-7b420e637b7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "EntityValidationError",
     "evalue": "Metadata content has failed validation for 'Sample 1':\n\t- characteristics-->geographic location (country and/or sea)-->0-->text: Input should be 'Afghanistan', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Arctic Ocean', 'Argentina', 'Armenia', 'Aruba', 'Ashmore and Cartier Islands', 'Atlantic Ocean', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Baker Island', 'Baltic Sea', 'Bangladesh', 'Barbados', 'Bassas da India', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Borneo', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Virgin Islands', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Clipperton Island', 'Cocos Islands', 'Colombia', 'Comoros', 'Cook Islands', 'Coral Sea Islands', 'Costa Rica', \"Cote d'Ivoire\", 'Croatia', 'Cuba', 'Curacao', 'Cyprus', 'Czechia', 'Democratic Republic of the Congo', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'East Timor', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Europa Island', 'Falkland Islands (Islas Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern and Antarctic Lands', 'Gabon', 'Gambia', 'Gaza Strip', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Glorioso Islands', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Honduras', 'Hong Kong', 'Howland Island', 'Hungary', 'Iceland', 'India', 'Indian Ocean', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Jan Mayen', 'Japan', 'Jarvis Island', 'Jersey', 'Johnston Atoll', 'Jordan', 'Juan de Nova Island', 'Kazakhstan', 'Kenya', 'Kerguelen Archipelago', 'Kingman Reef', 'Kiribati', 'Kosovo', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macau', 'Macedonia', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mediterranean Sea', 'Mexico', 'Micronesia', 'Midway Islands', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Navassa Island', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'North Korea', 'North Sea', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pacific Ocean', 'Pakistan', 'Palau', 'Palmyra Atoll', 'Panama', 'Papua New Guinea', 'Paracel Islands', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn Islands', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Republic of the Congo', 'Reunion', 'Romania', 'Ross Sea', 'Russia', 'Rwanda', 'Saint Helena', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'South Korea', 'Southern Ocean', 'Spain', 'Spratly Islands', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard', 'Swaziland', 'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Tajikistan', 'Tanzania', 'Tasman Sea', 'Thailand', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tromelin Island', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'USA', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Viet Nam', 'Virgin Islands', 'Wake Island', 'Wallis and Futuna', 'West Bank', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe', 'missing: control sample', 'missing: data agreement established pre-2023', 'missing: endangered species', 'missing: human-identifiable', 'missing: lab stock', 'missing: sample group', 'missing: synthetic construct', 'missing: third party data', 'not applicable', 'not collected', 'not provided' or 'restricted access'. Provided value: 'Mushroom kingdom'\n\t- characteristics-->geographic location (latitude)-->0-->text: Input should be a valid string. Provided value: '1.2234'\n\t- characteristics-->geographic location (longitude)-->0-->text: Input should be a valid string. Provided value: '7.21'\n\t- characteristics: Missing mandatory field 'elevation'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'Homo sapiens'}], 'project name': [{'text': 'Your fake project'}], 'collection date': [{'text': '2024-09-01'}], 'geographic location (country and/or sea)': [{'text': 'Mushroom kingdom'}], 'geographic location (latitude)': [{'text': 1.2234}], 'geographic location (longitude)': [{'text': 7.21}], 'broad-scale environmental context': [{'text': 'United Kingdom weather'}], 'local environmental context': [{'text': 'Mostly rainy'}], 'environmental medium': [{'text': 'Please read my plant'}]}'\n\t- characteristics: Missing mandatory field 'depth'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'Homo sapiens'}], 'project name': [{'text': 'Your fake project'}], 'collection date': [{'text': '2024-09-01'}], 'geographic location (country and/or sea)': [{'text': 'Mushroom kingdom'}], 'geographic location (latitude)': [{'text': 1.2234}], 'geographic location (longitude)': [{'text': 7.21}], 'broad-scale environmental context': [{'text': 'United Kingdom weather'}], 'local environmental context': [{'text': 'Mostly rainy'}], 'environmental medium': [{'text': 'Please read my plant'}]}'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEntityValidationError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample_checklist_validation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGscMixsSoil\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/biobroker/biobroker/metadata_entity/metadata_entity.py:75\u001b[0m, in \u001b[0;36mGenericEntity.validate\u001b[0;34m(self, data_model)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity \u001b[38;5;241m=\u001b[39m data_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity)\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_unset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, by_alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pydantic_core\u001b[38;5;241m.\u001b[39mValidationError \u001b[38;5;28;01mas\u001b[39;00m pydantic_error:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntityValidationError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger, entity_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, errors\u001b[38;5;241m=\u001b[39mpydantic_error\u001b[38;5;241m.\u001b[39merrors()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mEntityValidationError\u001b[0m: Metadata content has failed validation for 'Sample 1':\n\t- characteristics-->geographic location (country and/or sea)-->0-->text: Input should be 'Afghanistan', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Arctic Ocean', 'Argentina', 'Armenia', 'Aruba', 'Ashmore and Cartier Islands', 'Atlantic Ocean', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Baker Island', 'Baltic Sea', 'Bangladesh', 'Barbados', 'Bassas da India', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Borneo', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Virgin Islands', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Clipperton Island', 'Cocos Islands', 'Colombia', 'Comoros', 'Cook Islands', 'Coral Sea Islands', 'Costa Rica', \"Cote d'Ivoire\", 'Croatia', 'Cuba', 'Curacao', 'Cyprus', 'Czechia', 'Democratic Republic of the Congo', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'East Timor', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Europa Island', 'Falkland Islands (Islas Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern and Antarctic Lands', 'Gabon', 'Gambia', 'Gaza Strip', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Glorioso Islands', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Honduras', 'Hong Kong', 'Howland Island', 'Hungary', 'Iceland', 'India', 'Indian Ocean', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Jan Mayen', 'Japan', 'Jarvis Island', 'Jersey', 'Johnston Atoll', 'Jordan', 'Juan de Nova Island', 'Kazakhstan', 'Kenya', 'Kerguelen Archipelago', 'Kingman Reef', 'Kiribati', 'Kosovo', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macau', 'Macedonia', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mediterranean Sea', 'Mexico', 'Micronesia', 'Midway Islands', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Navassa Island', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'North Korea', 'North Sea', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pacific Ocean', 'Pakistan', 'Palau', 'Palmyra Atoll', 'Panama', 'Papua New Guinea', 'Paracel Islands', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn Islands', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Republic of the Congo', 'Reunion', 'Romania', 'Ross Sea', 'Russia', 'Rwanda', 'Saint Helena', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'South Korea', 'Southern Ocean', 'Spain', 'Spratly Islands', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard', 'Swaziland', 'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Tajikistan', 'Tanzania', 'Tasman Sea', 'Thailand', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tromelin Island', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'USA', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Viet Nam', 'Virgin Islands', 'Wake Island', 'Wallis and Futuna', 'West Bank', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe', 'missing: control sample', 'missing: data agreement established pre-2023', 'missing: endangered species', 'missing: human-identifiable', 'missing: lab stock', 'missing: sample group', 'missing: synthetic construct', 'missing: third party data', 'not applicable', 'not collected', 'not provided' or 'restricted access'. Provided value: 'Mushroom kingdom'\n\t- characteristics-->geographic location (latitude)-->0-->text: Input should be a valid string. Provided value: '1.2234'\n\t- characteristics-->geographic location (longitude)-->0-->text: Input should be a valid string. Provided value: '7.21'\n\t- characteristics: Missing mandatory field 'elevation'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'Homo sapiens'}], 'project name': [{'text': 'Your fake project'}], 'collection date': [{'text': '2024-09-01'}], 'geographic location (country and/or sea)': [{'text': 'Mushroom kingdom'}], 'geographic location (latitude)': [{'text': 1.2234}], 'geographic location (longitude)': [{'text': 7.21}], 'broad-scale environmental context': [{'text': 'United Kingdom weather'}], 'local environmental context': [{'text': 'Mostly rainy'}], 'environmental medium': [{'text': 'Please read my plant'}]}'\n\t- characteristics: Missing mandatory field 'depth'. Provided value: '{'collected_at': [{'text': 'noon'}], 'organism': [{'text': 'Homo sapiens'}], 'project name': [{'text': 'Your fake project'}], 'collection date': [{'text': '2024-09-01'}], 'geographic location (country and/or sea)': [{'text': 'Mushroom kingdom'}], 'geographic location (latitude)': [{'text': 1.2234}], 'geographic location (longitude)': [{'text': 7.21}], 'broad-scale environmental context': [{'text': 'United Kingdom weather'}], 'local environmental context': [{'text': 'Mostly rainy'}], 'environmental medium': [{'text': 'Please read my plant'}]}'"
     ]
    }
   ],
   "source": [
    "sample_checklist_validation.validate(GscMixsSoil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d972ace-ee90-4f5a-831a-3e6f78bff6ab",
   "metadata": {},
   "source": [
    "As we can see, it's complaining about a series of value errors. These value errors differ a bit from the value errors we saw in notebook 2: that is due to the **automatically-generated** model being a bit more constrained that the Biosamples checks. We could modify the model slightly to make it work better, but for the sake of the notebook, let's just input the values as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc9cf1f-c9c9-4797-a2ec-7d38e9a26579",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_checklist_validation['geographic location (country and/or sea)'] = \"Mediterranean Sea\"\n",
    "sample_checklist_validation['geographic location (latitude)'] = str(sample_checklist_validation['geographic location (latitude)']['text'])\n",
    "sample_checklist_validation['geographic location (longitude)'] = str(sample_checklist_validation['geographic location (latitude)']['text'])\n",
    "sample_checklist_validation['elevation'] = \"1\"\n",
    "sample_checklist_validation['depth'] = \"1\"\n",
    "\n",
    "sample_checklist_validation.validate(GscMixsSoil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838ff73-b4d9-4586-9cbe-d568e206270f",
   "metadata": {},
   "source": [
    "And now it has validated! Hoorray! It should be **ready** to submit, so let's just try that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b26a23-4c8b-4ede-8b33-f3b205748ea1",
   "metadata": {},
   "outputs": [
    {
     "ename": "BiosamplesValidationError",
     "evalue": "Found following errors in sample validation:\n\t- geographic location (latitude)/0.unit: should have required property 'unit'\n\t- geographic location (longitude)/0.unit: should have required property 'unit'\n\t- elevation/0.unit: should have required property 'unit'\n\t- depth/0.unit: should have required property 'unit')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBiosamplesValidationError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_checklist_validation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchecklist\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERC000022\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# We still need to tell BSD that we want to validate our samples!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m submitted_checklist_sample \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_checklist_validation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi\u001b[38;5;241m.\u001b[39mbase_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmitted_checklist_sample\u001b[38;5;241m.\u001b[39maccession\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/biobroker/biobroker/api/api.py:51\u001b[0m, in \u001b[0;36mGenericApi.submit\u001b[0;34m(self, entities, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_multiple(entities, kwargs)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_submit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentities\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/PycharmProjects/biobroker/biobroker/api/api.py:172\u001b[0m, in \u001b[0;36mBsdApi._submit\u001b[0;34m(self, entity, kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauthenticator\u001b[38;5;241m.\u001b[39mpost(submit_url, payload\u001b[38;5;241m=\u001b[39mentity\u001b[38;5;241m.\u001b[39mentity)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_submit_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Biosample(r\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/PycharmProjects/biobroker/biobroker/api/api.py:390\u001b[0m, in \u001b[0;36mBsdApi._submit_errors\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataPath\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BiosamplesValidationError(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BiosamplesNoErrorMessageError(response\u001b[38;5;241m.\u001b[39mstatus_code, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger)\n",
      "\u001b[0;31mBiosamplesValidationError\u001b[0m: Found following errors in sample validation:\n\t- geographic location (latitude)/0.unit: should have required property 'unit'\n\t- geographic location (longitude)/0.unit: should have required property 'unit'\n\t- elevation/0.unit: should have required property 'unit'\n\t- depth/0.unit: should have required property 'unit')"
     ]
    }
   ],
   "source": [
    "sample_checklist_validation['checklist'] = 'ERC000022' # We still need to tell BSD that we want to validate our samples!\n",
    "submitted_checklist_sample = api.submit([sample_checklist_validation])\n",
    "print(f\"{api.base_uri}/{submitted_checklist_sample.accession}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040fdcc0-ea6b-46cc-904b-b1c7c20f82dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'characteristics': {'project name': [{'text': 'Your fake project'}], 'collection date': [{'text': '2024-09-01'}], 'geographic location (country and/or sea)': [{'text': <Text5.Mediterranean_Sea: 'Mediterranean Sea'>}], 'geographic location (latitude)': [{'text': '1.2234'}], 'geographic location (longitude)': [{'text': '1.2234'}], 'broad-scale environmental context': [{'text': 'United Kingdom weather'}], 'local environmental context': [{'text': 'Mostly rainy'}], 'environmental medium': [{'text': 'Please read my plant'}], 'elevation': [{'text': '1'}], 'depth': [{'text': '1'}]}, 'name': 'Sample 1'}\n"
     ]
    }
   ],
   "source": [
    "print(sample_checklist_validation.entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf6e15-4ea5-4538-aeb4-583ce5a372ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "GscMixsSoil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
