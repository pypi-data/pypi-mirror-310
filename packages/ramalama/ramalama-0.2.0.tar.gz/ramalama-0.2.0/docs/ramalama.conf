# The RamaLama configuration file specifies all of the available configuration
# command-line options/flags for container engine tools like Podman & Buildah,
# but in a TOML format that can be easily modified and versioned.

# Please refer to ramalama.conf(5) for details of all configuration options.
# Not all container engines implement all of the options.
# All of the options have hard coded defaults and these options will override
# the built in defaults. Users can then override these options via the command
# line. Container engines will read ramalama.conf files in up to three
# locations in the following order:
#  1. /usr/share/ramalama/ramalama.conf
#  2. /etc/ramalama/ramalama.conf
#  3. $XDG_CONFIG_HOME/ramalama/ramalama.conf or
#     $HOME/.config/ramalama/ramalama.conf if $XDG_CONFIG_HOME is not set
#  Items specified in the latter ramalama.conf, if they exist, override the
# previous ramalama.conf settings, or the default settings.

[ramalama]

# Run RamaLama in the default container.
#
#container = true

# OCI Model Car image
# Image to use when building and pushing --type=car models
#
#carimage = "registry.access.redhat.com/ubi9-micro:latest"

# Run RamaLama using the specified container engine.
#
# Valid options (Podman, Docker)
#engine = "podman"

# OCI container image to run with the specified AI model
#
#image = "quay.io/ramalama/ramalama:latest"

# IP address for llama.cpp to listen on.
#
#host = "0.0.0.0"

# Specify default port for services to listen on
#
#port = "8080"

# Specify the AI runtime to use; valid options are 'llama.cpp' and 'vllm' (default: llama.cpp)
# Options: llama.cpp, vllm
#
#runtime = "llama.cpp"

# Store AI Models in the specified directory
#
#store = "$HOME/.local/share/ramalama"

# Specify the default transport to be used for pulling and pushing of AI Models.
# Options: oci, ollama, huggingface.
#
#transport = "ollama"
