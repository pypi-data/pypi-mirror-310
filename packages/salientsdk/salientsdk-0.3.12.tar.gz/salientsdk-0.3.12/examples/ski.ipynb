{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salient Predictions Ski-Cast\n",
    "\n",
    "In November, Salient predicts snow accumulation at 90 IKON and Epic resorts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7f3257844510>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    import salientsdk as sk\n",
    "except ModuleNotFoundError as e:\n",
    "    if os.path.exists(\"../salientsdk\"):\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        import salientsdk as sk\n",
    "    else:\n",
    "        raise ModuleNotFoundError(\"Install salient SDK with: pip install salientsdk\")\n",
    "\n",
    "force = False\n",
    "\n",
    "year = datetime.datetime.now().year\n",
    "today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "start_month = 10  # first day of the ski season\n",
    "end_month = 5  # final day of the ski season\n",
    "fcst_date = f\"{year}-10-15\"\n",
    "hist_start = f\"1990-{start_month}-01\"\n",
    "fcst_end = f\"{year+1}-{end_month}-01\"\n",
    "vars = [\"temp\", \"precip\"]\n",
    "\n",
    "\n",
    "sk.set_file_destination(\"ski_example\")\n",
    "sk.login(\"username\", \"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "resorts = {\n",
    "    \"japan\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": 137.861, \"lat\": 36.690, \"name\": \"Hakuba\"},  # epic\n",
    "            {\"lon\": 140.687, \"lat\": 42.824, \"name\": \"Rusutsu\"},  # epic\n",
    "            {\"lon\": 140.685, \"lat\": 42.824, \"name\": \"Niseko\"},  # ikon\n",
    "            {\"lon\": 140.685, \"lat\": 42.824, \"name\": \"Lotte Arai\"},  # ikon\n",
    "            # Tazawako indy\n",
    "            # Okunakayama\n",
    "        ]\n",
    "    ),\n",
    "    \"alps\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": 6.8632749, \"lat\": 45.924065, \"name\": \"Chamonix\"},  # ikon\n",
    "            {\"lon\": 7.7522747, \"lat\": 46.0222204, \"name\": \"Zermatt\"},  # ikon\n",
    "            {\"lon\": 8.5916293, \"lat\": 46.6324621, \"name\": \"Andermatt-Sedrun\"},  # epic\n",
    "            {\"lon\": 12.3925407, \"lat\": 47.4492375, \"name\": \"Kitzbuhel\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"pnw\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": -123.204545, \"lat\": 49.396018, \"name\": \"Cypress\"},  # ikon\n",
    "            {\"lon\": -121.6781891, \"lat\": 44.0024701, \"name\": \"Bachelor\"},  # ikon\n",
    "            {\"lon\": -121.0890197, \"lat\": 47.7448119, \"name\": \"Stevens Pass\"},  # epic\n",
    "            {\"lon\": -122.9486474, \"lat\": 50.1149639, \"name\": \"Whistler\"},  # epic\n",
    "            {\"lon\": -121.4747533, \"lat\": 46.9352963, \"name\": \"Crystal Mtn\"},  # ikon\n",
    "            {\"lon\": -121.4257485, \"lat\": 47.4442426, \"name\": \"Alpental\"},  # ikon\n",
    "            {\"lon\": -121.4164161, \"lat\": 47.4245711, \"name\": \"Snoqualmie\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"rockies\": pd.DataFrame(\n",
    "        [\n",
    "            # The four Aspen resorts all perform similarly.  Combine into one:\n",
    "            # {\"lon\": -106.9490961, \"lat\": 39.2083984, \"name\": \"Aspen Snowmass\"},\n",
    "            # {\"lon\": -106.8610687, \"lat\": 39.2058029, \"name\": \"Buttermilk\"},\n",
    "            # {\"lon\": -106.8553613, \"lat\": 39.1824124, \"name\": \"Aspen Highlands\"},\n",
    "            {\"lon\": -106.818227, \"lat\": 39.1862601, \"name\": \"Aspen Mtn\"},  # ikon\n",
    "            {\"lon\": -106.8045169, \"lat\": 40.4571991, \"name\": \"Steamboat\"},  # ikon\n",
    "            # Beaver Creek and Vail perform similarly and are right next to each other\n",
    "            # {\"lon\": -106.5167109, \"lat\": 39.6042863, \"name\": \"Beaver Creek\"}, # epic\n",
    "            {\"lon\": -106.3549717, \"lat\": 39.6061444, \"name\": \"Vail\"},  # epic\n",
    "            {\"lon\": -106.1516265, \"lat\": 39.501419, \"name\": \"Copper\"},  # ikon\n",
    "            {\"lon\": -106.0676088, \"lat\": 39.4808351, \"name\": \"Breckenridge\"},  # epic\n",
    "            {\"lon\": -105.9437656, \"lat\": 39.6075962, \"name\": \"Keystone\"},  # epic\n",
    "            {\"lon\": -105.8719397, \"lat\": 39.6425118, \"name\": \"A-Basin\"},  # ikon\n",
    "            {\"lon\": -105.762488, \"lat\": 39.8868392, \"name\": \"Winter Park\"},  # ikon\n",
    "            {\"lon\": -105.5826786, \"lat\": 39.9372203, \"name\": \"Eldora\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"new_england\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": -72.9278443, \"lat\": 43.0906207, \"name\": \"Stratton\"},  # ikon\n",
    "            {\"lon\": -72.9204014, \"lat\": 42.9602444, \"name\": \"Mt Snow\"},  # epic\n",
    "            {\"lon\": -72.8944139, \"lat\": 44.1359019, \"name\": \"Sugarbush\"},  # ikon\n",
    "            # {\"lon\": -72.842512, \"lat\": 43.6621499, \"name\": \"Pico\"}, # ikon near Killington\n",
    "            {\"lon\": -72.7967531, \"lat\": 43.6262922, \"name\": \"Killington\"},  # ikon\n",
    "            {\"lon\": -72.7814124, \"lat\": 44.5303066, \"name\": \"Stowe\"},  # epic\n",
    "            {\"lon\": -72.7170416, \"lat\": 43.4018257, \"name\": \"Okemo\"},  # epic\n",
    "            {\"lon\": -72.08014, \"lat\": 43.331889, \"name\": \"Sunapee\"},  # epic\n",
    "            {\"lon\": -71.8655176, \"lat\": 43.0198715, \"name\": \"Crotched\"},  # epic\n",
    "            {\"lon\": -71.6336041, \"lat\": 44.0563456, \"name\": \"Loon\"},  # ikon\n",
    "            {\"lon\": -71.2393036, \"lat\": 44.2640724, \"name\": \"Wildcat\"},  # epic\n",
    "            {\"lon\": -71.229443, \"lat\": 44.082771, \"name\": \"Attitash\"},  # epic\n",
    "            {\"lon\": -70.8568727, \"lat\": 44.4734182, \"name\": \"Sunday River\"},  # ikon\n",
    "            {\"lon\": -70.3085109, \"lat\": 45.0541811, \"name\": \"Sugarloaf\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"europe\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": 1.4707674, \"lat\": 42.5729217, \"name\": \"Arinsal\"},  # ikon\n",
    "            {\"lon\": 1.499825, \"lat\": 42.6317345, \"name\": \"Ordino Arcal√≠s\"},  # ikon\n",
    "            {\"lon\": 1.6462281, \"lat\": 42.5783833, \"name\": \"Grandvalira\"},  # ikon\n",
    "            {\"lon\": 11.6520936, \"lat\": 46.5739752, \"name\": \"Dolomiti\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"na_west\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": -120.2483913, \"lat\": 39.1906091, \"name\": \"Palisades Tahoe\"},  # ikon\n",
    "            {\"lon\": -120.1210934, \"lat\": 39.2745678, \"name\": \"Northstar\"},  # epic\n",
    "            {\"lon\": -120.0651665, \"lat\": 38.6847514, \"name\": \"Kirkwood\"},  # epic\n",
    "            {\"lon\": -119.9428424, \"lat\": 38.9569241, \"name\": \"Heavenly\"},  # epic\n",
    "            {\"lon\": -119.8859331, \"lat\": 50.8844311, \"name\": \"Sun Peaks\"},  # ikon\n",
    "            {\"lon\": -119.0906293, \"lat\": 37.7679169, \"name\": \"June\"},  # ikon\n",
    "            {\"lon\": -119.0267806, \"lat\": 37.6510972, \"name\": \"Mammoth\"},  # ikon\n",
    "            {\"lon\": -118.1630779, \"lat\": 50.9583858, \"name\": \"Revelstoke\"},  # ikon\n",
    "            {\"lon\": -117.8194705, \"lat\": 49.1024147, \"name\": \"RED\"},  # ikon\n",
    "            {\"lon\": -117.036177, \"lat\": 34.2248821, \"name\": \"Snow Valley\"},  # ikon\n",
    "            {\"lon\": -116.8892717, \"lat\": 34.2364081, \"name\": \"Snow Summit\"},  # ikon\n",
    "            {\"lon\": -116.8608572, \"lat\": 34.2276766, \"name\": \"Bear Mtn\"},  # ikon\n",
    "            {\"lon\": -116.6227441, \"lat\": 48.3679757, \"name\": \"Schweitzer\"},  # ikon\n",
    "            {\"lon\": -116.2380671, \"lat\": 50.4602801, \"name\": \"Panorama\"},  # ikon\n",
    "            {\"lon\": -116.1621717, \"lat\": 51.4419206, \"name\": \"Lake Louise\"},  # ikon\n",
    "            {\"lon\": -115.7840699, \"lat\": 51.0780997, \"name\": \"Banff\"},  # ikon\n",
    "            {\"lon\": -115.5982699, \"lat\": 51.2037624, \"name\": \"Norquay\"},  # ikon\n",
    "            {\"lon\": -115.5707632, \"lat\": 51.1751675, \"name\": \"SkiBig3\"},  # ikon\n",
    "            {\"lon\": -114.3542874, \"lat\": 43.6949128, \"name\": \"Sun Valley\"},  # ikon\n",
    "            {\"lon\": -114.3461537, \"lat\": 43.6820566, \"name\": \"Dollar Mtn\"},  # ikon\n",
    "            {\"lon\": -111.8571529, \"lat\": 41.2161404, \"name\": \"Snowbasin\"},  # ikon\n",
    "            # The four Cottonwood Canyon resorts all perform similarly.  Combine:\n",
    "            # {\"lon\": -111.6563885, \"lat\": 40.5810814, \"name\": \"Snowbird\"},\n",
    "            {\"lon\": -111.6385807, \"lat\": 40.5884218, \"name\": \"Alta\"},  # ikon\n",
    "            # {\"lon\": -111.591885, \"lat\": 40.619852, \"name\": \"Solitude\"},\n",
    "            # {\"lon\": -111.583187, \"lat\": 40.598019, \"name\": \"Brighton\"},\n",
    "            {\"lon\": -111.5079947, \"lat\": 40.6514199, \"name\": \"Park City\"},  # epic\n",
    "            {\"lon\": -111.478306, \"lat\": 40.63738, \"name\": \"Deer Valley\"},  # ikon\n",
    "            {\"lon\": -111.4012076, \"lat\": 45.2857289, \"name\": \"Big Sky\"},  # ikon\n",
    "            {\"lon\": -110.8279183, \"lat\": 43.5875453, \"name\": \"Jackson Hole\"},  # ikon\n",
    "            {\"lon\": -106.9878231, \"lat\": 38.8697146, \"name\": \"Crested Butte\"},  # ikon\n",
    "            {\"lon\": -105.4545, \"lat\": 36.5959999, \"name\": \"Taos\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"na_east\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": -94.9707416, \"lat\": 39.4673048, \"name\": \"Snow Creek\"},  # epic- Kansas City\n",
    "            {\"lon\": -92.7878062, \"lat\": 44.8576608, \"name\": \"Afton\"},  # epic - Minneapolis\n",
    "            {\"lon\": -90.6506898, \"lat\": 38.5353168, \"name\": \"Hidden Valley\"},  # epic\n",
    "            {\"lon\": -88.1876602, \"lat\": 42.4989548, \"name\": \"Wilmot\"},  # epic\n",
    "            {\"lon\": -86.5122305, \"lat\": 38.5555868, \"name\": \"Paoli\"},  # epic\n",
    "            {\"lon\": -84.930067, \"lat\": 45.162884, \"name\": \"Boyne\"},  # ikon\n",
    "            # {\"lon\": -84.926535, \"lat\": 45.4647239, \"name\": \"Boyne Highlands\"},  # ikon\n",
    "            {\"lon\": -83.8115217, \"lat\": 42.54083, \"name\": \"Mt. Brighton\"},  # epic\n",
    "            {\"lon\": -83.6777778, \"lat\": 40.3180556, \"name\": \"Mad River\"},  # epic\n",
    "            {\"lon\": -81.5632108, \"lat\": 41.2640987, \"name\": \"Boston Mills\"},  # epic\n",
    "            {\"lon\": -81.259745, \"lat\": 41.52687, \"name\": \"Alpine Valley\"},  # epic\n",
    "            {\"lon\": -80.3122216, \"lat\": 44.5037818, \"name\": \"Blue Mtn\"},  # ikon\n",
    "            {\"lon\": -79.9960444, \"lat\": 38.4118566, \"name\": \"Snowshoe\"},  # ikon\n",
    "            {\"lon\": -79.2977032, \"lat\": 40.0229768, \"name\": \"7 Springs\"},  # epic\n",
    "            {\"lon\": -79.2581204, \"lat\": 40.058031, \"name\": \"Hidden Valley 2\"},  # epic\n",
    "            {\"lon\": -79.1657908, \"lat\": 40.1638728, \"name\": \"Laurel\"},  # epic\n",
    "            {\"lon\": -77.9333126, \"lat\": 39.7417652, \"name\": \"Whitetail\"},  # epic\n",
    "            {\"lon\": -77.375459, \"lat\": 39.76366, \"name\": \"Liberty\"},  # epic\n",
    "            {\"lon\": -76.9275492, \"lat\": 40.1094506, \"name\": \"Roundtop\"},  # epic\n",
    "            {\"lon\": -75.6563315, \"lat\": 41.1091686, \"name\": \"Jack Frost\"},  # epic\n",
    "            {\"lon\": -75.601282, \"lat\": 41.050189, \"name\": \"Big Boulder\"},  # epic\n",
    "            {\"lon\": -74.5852526, \"lat\": 46.2096417, \"name\": \"Tremblant\"},  # ikon\n",
    "            {\"lon\": -74.2567116, \"lat\": 42.2937298, \"name\": \"Windham\"},  # ikon\n",
    "            {\"lon\": -74.2246402, \"lat\": 42.2028811, \"name\": \"Hunter\"},  # epic\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Assign a color to each region for later plotting purposes, using\n",
    "# Tol's colorblind-friendly \"vibrant\" palette.\n",
    "# https://cran.r-project.org/web/packages/khroma/vignettes/tol.html\n",
    "colors = {\n",
    "    \"japan\": \"#004488\",  # blue\n",
    "    \"alps\": \"#33BBEE\",  # cyan\n",
    "    \"pnw\": \"#009988\",  # teal\n",
    "    \"rockies\": \"#CC3311\",  # red\n",
    "    \"new_england\": \"#DDAA33\",  # yellow\n",
    "    \"europe\": \"#555555\",  # dark grey\n",
    "    \"na_west\": \"#666666\",  # grey\n",
    "    \"na_east\": \"#777777\",  # light grey\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location file: ['ski_resorts_japan.csv', 'ski_resorts_alps.csv', 'ski_resorts_pnw.csv', 'ski_resorts_rockies.csv', 'ski_resorts_new_england.csv', 'ski_resorts_europe.csv', 'ski_resorts_na_west.csv', 'ski_resorts_na_east.csv']\n"
     ]
    }
   ],
   "source": [
    "geo_files = {\n",
    "    region: sk.upload_location_file(\n",
    "        lats=geo[\"lat\"],\n",
    "        lons=geo[\"lon\"],\n",
    "        names=geo[\"name\"],\n",
    "        geoname=f\"ski_resorts_{region}\",\n",
    "        force=force,\n",
    "    )\n",
    "    for region, geo in resorts.items()\n",
    "}\n",
    "\n",
    "# We will later use a Location object to query the Salient API\n",
    "# The functions are capable of handling multiple location files,\n",
    "# so we can pass a vector here.\n",
    "ski_locs = sk.Location(location_file=list(geo_files.values()))\n",
    "print(ski_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire the data\n",
    "\n",
    "For each of the ski resorts, we will get the daily forecast of temperature and precipitation as of the beginning of the season. Then we will also get the historical observed conditions, calculate snowfall, and merge them into a single dataset for later analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Downscale Forecast\n",
    "\n",
    "In contrast to the probabilistic `forecast_timeseries` function, `downscale` samples historical analogs from the forecast distribution to create ensemble timeseries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 7MB\n",
      "Dimensions:        (time: 199, ensemble: 51, location: 92)\n",
      "Coordinates:\n",
      "  * time           (time) datetime64[ns] 2kB 2024-10-15 ... 2025-05-01\n",
      "  * location       (location) object 736B 'Hakuba' 'Rusutsu' ... 'Hunter'\n",
      "    lat            (location) float64 736B 36.69 42.82 42.82 ... 42.29 42.2\n",
      "    lon            (location) float64 736B 137.9 140.7 140.7 ... -74.26 -74.22\n",
      "    forecast_date  datetime64[ns] 8B 2024-10-15\n",
      "    season         (time) object 2kB 2024 2024 2024 2024 ... 2024 2024 2024 <NA>\n",
      "    season_day     (time) object 2kB 14 15 16 17 18 19 ... 208 209 210 211 <NA>\n",
      "Dimensions without coordinates: ensemble\n",
      "Data variables:\n",
      "    temp           (ensemble, time, location) float32 4MB 16.77 15.2 ... 9.413\n",
      "    precip         (ensemble, time, location) float32 4MB 0.0 23.1 ... 2.597\n"
     ]
    }
   ],
   "source": [
    "fcst_files = sk.downscale(\n",
    "    loc=ski_locs,\n",
    "    variables=vars,\n",
    "    members=51,\n",
    "    date=fcst_date,\n",
    "    force=force,\n",
    ")\n",
    "\n",
    "# Because we are requesting multiple location_files, fcst_files is a\n",
    "# table with multiple downscale files.  Let's combine all of them:\n",
    "fcst = xr.open_mfdataset(\n",
    "    fcst_files[\"file_name\"].values,\n",
    "    concat_dim=\"location\",\n",
    "    combine=\"nested\",\n",
    ")\n",
    "# Align the data to the ski season:\n",
    "fcst = fcst.sel(forecast_day=slice(fcst_date, fcst_end))\n",
    "\n",
    "# We use scientific units for precip like mm day-1\n",
    "# Let's make this more readable for plotting purposes:\n",
    "fcst[\"precip\"].attrs[\"units\"] = \"mm/day\"\n",
    "\n",
    "# rename \"forecast_day\" to \"time\" to match the output from data_timeseries\n",
    "fcst = fcst.rename({\"forecast_day\": \"time\"})\n",
    "\n",
    "# Remove things we don't need:\n",
    "fcst = fcst.drop_vars([\"temp_clim\", \"precip_clim\", \"temp_anom\", \"precip_anom\", \"analog\"])\n",
    "\n",
    "\n",
    "def add_season(ds, season_start_month=10, season_end_month=5):\n",
    "    \"\"\"Add season and season_day coordinates to a dataset based on time.\n",
    "\n",
    "    Args:\n",
    "        ds: xarray Dataset with a 'time' coordinate\n",
    "        season_start_month: Month when season starts (e.g. 10 for October)\n",
    "        season_end_month: Month when season ends (e.g. 5 for May)\n",
    "\n",
    "    Returns:\n",
    "        xarray Dataset with new 'season' and 'season_day' coordinates\n",
    "    \"\"\"\n",
    "    time = ds.time\n",
    "    month = time.dt.month.values\n",
    "    year = time.dt.year.values\n",
    "\n",
    "    # Calculate season_day relative to each year's season start\n",
    "    season_starts = pd.to_datetime([f\"{y}-{season_start_month:02d}-01\" for y in year])\n",
    "    time_np = time.values.astype(\"datetime64[D]\")\n",
    "    season_starts_np = season_starts.values.astype(\"datetime64[D]\")\n",
    "\n",
    "    # For dates before October, use previous year's October 1\n",
    "    # Account for leap years in the offset\n",
    "    is_leap = pd.to_datetime(season_starts_np).is_leap_year\n",
    "    year_days = np.where(is_leap, 366, 365)\n",
    "\n",
    "    days = (\n",
    "        (\n",
    "            time_np\n",
    "            - np.where(\n",
    "                month < season_start_month,\n",
    "                season_starts_np - np.timedelta64(1, \"D\") * year_days,\n",
    "                season_starts_np,\n",
    "            )\n",
    "        )\n",
    "        .astype(\"timedelta64[D]\")\n",
    "        .astype(int)\n",
    "    )\n",
    "    is_summer = (month >= season_end_month) & (month < season_start_month)\n",
    "    season_day = np.where(is_summer, pd.NA, days)\n",
    "\n",
    "    # Calculate season year\n",
    "    season = year\n",
    "    season = np.where(month < season_start_month, season - 1, season)\n",
    "    season = np.where(is_summer, pd.NA, season)\n",
    "\n",
    "    # Add new coordinates\n",
    "    ds = ds.assign_coords({\"season\": (\"time\", season), \"season_day\": (\"time\", season_day)})\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "fcst = add_season(fcst)\n",
    "\n",
    "\n",
    "fcst = fcst.compute()\n",
    "\n",
    "print(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Historical Data\n",
    "\n",
    "The `data_timeseries` function will load the historical daily ERA5 timeseries, which we can later compare to the `downscale` timeseries ensembles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 19MB\n",
      "Dimensions:        (time: 12458, location: 92)\n",
      "Coordinates:\n",
      "  * time           (time) datetime64[ns] 100kB 1990-10-01 ... 2024-11-08\n",
      "  * location       (location) object 736B '7 Springs' 'A-Basin' ... 'Zermatt'\n",
      "    lat            (location) float64 736B 40.02 39.64 44.86 ... 39.89 46.02\n",
      "    lon            (location) float64 736B -79.3 -105.9 -92.79 ... -105.8 7.752\n",
      "    location_file  (location) object 736B 'ski_resorts_na_east.csv' ... 'ski_...\n",
      "    season         (time) object 100kB 1990 1990 1990 1990 ... 2024 2024 2024\n",
      "    season_day     (time) object 100kB 0 1 2 3 4 5 6 7 ... 32 33 34 35 36 37 38\n",
      "    region         (location) <U11 4kB 'na_east' 'rockies' ... 'rockies' 'alps'\n",
      "    color          (location) <U7 3kB '#777777' '#CC3311' ... '#33BBEE'\n",
      "Data variables:\n",
      "    temp           (time, location) float64 9MB 10.52 6.065 ... -5.545 0.9684\n",
      "    precip         (time, location) float64 9MB 0.01205 0.2661 ... 6.513 0.07209\n"
     ]
    }
   ],
   "source": [
    "# Get historical observed performance for each ski resort\n",
    "hist_files = sk.data_timeseries(\n",
    "    loc=ski_locs,\n",
    "    variable=vars,\n",
    "    field=\"vals\",\n",
    "    start=hist_start,\n",
    "    end=min(fcst_end, today),\n",
    "    frequency=\"daily\",\n",
    "    force=force,\n",
    ")\n",
    "\n",
    "# Assemble each historical file into a single xarray dataset\n",
    "hist = sk.load_multihistory(hist_files)\n",
    "hist = add_season(hist)\n",
    "\n",
    "# Assign a color to each region for plotting purposes later\n",
    "prefix = \"ski_resorts_\"\n",
    "suffix = \".csv\"\n",
    "region = [x.replace(prefix, \"\").replace(suffix, \"\") for x in hist[\"location_file\"].values]\n",
    "regcol = [colors[reg] for reg in region]\n",
    "hist = hist.assign_coords(region=(\"location\", region), color=(\"location\", regcol))\n",
    "\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add history before forecast starts\n",
    "\n",
    "We begin analyzing each ski season in October to account for snow accumulation before the mountains become skiable. If the forecast was generated after October, prepend the observed history to the forecast timeseries so that we account for weather before the forecast was generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 16MB\n",
      "Dimensions:        (location: 92, ensemble: 51, time: 213)\n",
      "Coordinates:\n",
      "  * location       (location) object 736B '7 Springs' 'A-Basin' ... 'Zermatt'\n",
      "  * ensemble       (ensemble) int64 408B 0 1 2 3 4 5 6 ... 44 45 46 47 48 49 50\n",
      "  * time           (time) datetime64[ns] 2kB 2024-10-01 ... 2025-05-01\n",
      "    lat            (location) float64 736B 40.02 39.64 44.86 ... 39.89 46.02\n",
      "    lon            (location) float64 736B -79.3 -105.9 -92.79 ... -105.8 7.752\n",
      "    location_file  (location) object 736B 'ski_resorts_na_east.csv' ... 'ski_...\n",
      "    season         (time) object 2kB 2024 2024 2024 2024 ... 2024 2024 2024 <NA>\n",
      "    season_day     (time) object 2kB 0 1 2 3 4 5 6 ... 207 208 209 210 211 <NA>\n",
      "    region         (location) <U11 4kB 'na_east' 'rockies' ... 'rockies' 'alps'\n",
      "    color          (location) <U7 3kB '#777777' '#CC3311' ... '#33BBEE'\n",
      "    forecast_date  datetime64[ns] 8B 2024-10-15\n",
      "Data variables:\n",
      "    temp           (ensemble, time, location) float64 8MB 15.71 7.041 ... 1.195\n",
      "    precip         (ensemble, time, location) float64 8MB 22.07 ... 37.29\n"
     ]
    }
   ],
   "source": [
    "season_start = f\"{year}-{start_month}-01\"\n",
    "if fcst.time[0] > np.datetime64(season_start):\n",
    "    fcst = xr.concat(\n",
    "        [\n",
    "            hist.sel(time=slice(season_start, fcst.time[0] - pd.Timedelta(days=1)))\n",
    "            .expand_dims(ensemble=fcst.ensemble)\n",
    "            .transpose(\"ensemble\", \"time\", \"location\"),\n",
    "            fcst,\n",
    "        ],\n",
    "        dim=\"time\",\n",
    "    )\n",
    "    print(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Snow Water Equivalent\n",
    "\n",
    "The `calc_swe` function builds on the `snow17` model to calculate the snow water equivalent (SWE) at each location and for each ensemble. It requires that the dataset input has data values `precip` and `temp`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data variables:\n",
      "    temp     (ensemble, time, location) float64 8MB 15.71 7.041 ... -6.851 1.195\n",
      "    precip   (ensemble, time, location) float64 8MB 22.07 0.001423 ... 37.29\n",
      "    swe      (ensemble, time, location) float64 8MB 0.0 0.0 0.0 ... 272.6 576.1\n"
     ]
    }
   ],
   "source": [
    "if \"swe\" not in fcst:\n",
    "    fcst[\"swe\"] = sk.hydro.calc_swe(fcst, \"time\")\n",
    "\n",
    "print(fcst.data_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data variables:\n",
      "    temp     (time, location) float64 9MB 10.52 6.065 13.48 ... -5.545 0.9684\n",
      "    precip   (time, location) float64 9MB 0.01205 0.2661 2.719 ... 6.513 0.07209\n",
      "    swe      (time, location) float64 9MB 0.0 0.0 0.0 0.0 ... 0.0 32.8 0.03808\n"
     ]
    }
   ],
   "source": [
    "if \"swe\" not in hist:\n",
    "    hist[\"swe\"] = sk.hydro.calc_swe(hist, \"time\")\n",
    "\n",
    "print(hist.data_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate a seasonal average similar to the forecast's per-ensemble average we need to break the single linear `time` dimension into `season` + `season_day` dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data variables:\n",
      "    temp     (location, season, season_day) float64 5MB 10.52 12.35 ... nan nan\n",
      "    precip   (location, season, season_day) float64 5MB 0.01205 0.03876 ... nan\n",
      "    swe      (location, season, season_day) float64 5MB 0.0 0.0 0.0 ... nan nan\n",
      "    time     (season, season_day) datetime64[ns] 60kB 1990-10-01 ... NaT\n"
     ]
    }
   ],
   "source": [
    "def stack_by_season(ds):\n",
    "    \"\"\"Convert a dataset from time dimension to season/season_day dimensions.\n",
    "\n",
    "    Args:\n",
    "        ds: xarray Dataset with time dimension and season/season_day coordinates\n",
    "\n",
    "    Returns:\n",
    "        xarray Dataset with season and season_day dimensions instead of time\n",
    "    \"\"\"\n",
    "    valid_mask = ~ds[\"season\"].isnull() & ~ds[\"season_day\"].isnull()\n",
    "    ds_clean = ds.isel(time=valid_mask).copy()\n",
    "    ds_clean[\"season\"] = ds_clean.season.astype(int)\n",
    "    ds_clean[\"season_day\"] = ds_clean.season_day.astype(int)\n",
    "    time_df = pd.DataFrame(\n",
    "        {\n",
    "            \"season\": ds_clean.season.values,\n",
    "            \"season_day\": ds_clean.season_day.values,\n",
    "            \"time\": ds_clean.time.values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ds_stacked = ds_clean.set_index(time=[\"season\", \"season_day\"])\n",
    "    ds_final = ds_stacked.unstack(\"time\")\n",
    "    idx = pd.MultiIndex.from_product(\n",
    "        [ds_final.season.values, ds_final.season_day.values], names=[\"season\", \"season_day\"]\n",
    "    )\n",
    "    time_series = time_df.set_index([\"season\", \"season_day\"])[\"time\"].reindex(idx)\n",
    "    ds_final[\"time\"] = xr.DataArray(\n",
    "        time_series.values.reshape(len(ds_final.season), len(ds_final.season_day)),\n",
    "        dims=[\"season\", \"season_day\"],\n",
    "    )\n",
    "\n",
    "    return ds_final\n",
    "\n",
    "\n",
    "hist = stack_by_season(hist)\n",
    "# hist[\"swe_avg\"] = hist[\"swe\"].mean([\"season\", \"season_day\"])\n",
    "print(hist.data_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can later merge, let's similarly denominate the forecast by `season_day` instead of `time`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 24MB\n",
      "Dimensions:        (season_day: 212, location: 92, ensemble: 51)\n",
      "Coordinates:\n",
      "    season         int64 8B 2024\n",
      "  * season_day     (season_day) int64 2kB 0 1 2 3 4 5 ... 207 208 209 210 211\n",
      "  * location       (location) object 736B '7 Springs' 'A-Basin' ... 'Zermatt'\n",
      "  * ensemble       (ensemble) int64 408B 0 1 2 3 4 5 6 ... 44 45 46 47 48 49 50\n",
      "    lat            (location) float64 736B 40.02 39.64 44.86 ... 39.89 46.02\n",
      "    lon            (location) float64 736B -79.3 -105.9 -92.79 ... -105.8 7.752\n",
      "    location_file  (location) object 736B 'ski_resorts_na_east.csv' ... 'ski_...\n",
      "    region         (location) <U11 4kB 'na_east' 'rockies' ... 'rockies' 'alps'\n",
      "    color          (location) <U7 3kB '#777777' '#CC3311' ... '#33BBEE'\n",
      "    forecast_date  datetime64[ns] 8B 2024-10-15\n",
      "Data variables:\n",
      "    temp           (ensemble, location, season_day) float64 8MB 15.71 ... 3.384\n",
      "    precip         (ensemble, location, season_day) float64 8MB 22.07 ... 4.798\n",
      "    swe            (ensemble, location, season_day) float64 8MB 0.0 ... 561.2\n",
      "    time           (season_day) datetime64[ns] 2kB 2024-10-01 ... 2025-04-30\n"
     ]
    }
   ],
   "source": [
    "fcst = stack_by_season(fcst).squeeze(\"season\")\n",
    "print(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Anomalies\n",
    "\n",
    "Combine the historical and forecast datasets into a single dataset so that we can make sure they are aligned by `location`.\n",
    "\n",
    "We don't want to highlight resorts with below-average snowfall, so let's sort the dataset and cut out the bottom half.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define \"climatology\" as the most recent 15 years:\n",
    "clim = hist.sel(season=slice(year - 16, year - 1)).mean(\"season\", keep_attrs=True)\n",
    "clim = clim.rolling(season_day=21, center=True, min_periods=1).mean()\n",
    "\n",
    "# Let's focus our analysis on winter solstice to spring break, when most people ski\n",
    "high_season = slice(81, 170)\n",
    "# We're only going to plot locations with above-average snowfall\n",
    "loc_avg = fcst.swe.mean(dim=[\"ensemble\", \"season_day\"])\n",
    "best_locations = loc_avg[loc_avg > 0.5 * loc_avg.mean()].location.values\n",
    "\n",
    "with xr.set_options(keep_attrs=True):\n",
    "    fcst_sub = fcst.sel(season_day=high_season, location=best_locations).mean(\"season_day\")\n",
    "    clim_sub = clim.sel(season_day=high_season, location=best_locations).mean(\"season_day\")\n",
    "    # Convert temperature to degK so we can analyze as ratios\n",
    "    fcst_sub[\"temp\"] = fcst_sub[\"temp\"] + 273\n",
    "    clim_sub[\"temp\"] = clim_sub[\"temp\"] + 273\n",
    "\n",
    "    anom = 100 * fcst_sub / clim_sub\n",
    "    anomd = fcst_sub - clim_sub\n",
    "\n",
    "# Sort by SWE anomaly, highest first\n",
    "locs_sort = anom.swe.mean(dim=[\"ensemble\"]).sortby(anom.swe.mean(dim=[\"ensemble\"])).location\n",
    "anom = anom.sel(location=locs_sort)\n",
    "anomd = anomd.sel(location=locs_sort)\n",
    "\n",
    "[\n",
    "    anom[var].attrs.update(\n",
    "        {\"units\": \"%\", \"long_name\": f'{anom[var].attrs.get(\"long_name\", var)} anomaly'}\n",
    "    )\n",
    "    for var in anom.data_vars\n",
    "];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes(\n",
    "    fcst: xr.DataArray,\n",
    "    title: str = \"\",\n",
    "    vline=None,\n",
    "    legend_loc: str = \"center right\",\n",
    "    ax=None,\n",
    "):\n",
    "    \"\"\"Plot predictions and observed values as a box-and-whisker plot.\"\"\"\n",
    "    # extract a table of seasonal averages (across all days) per location\n",
    "    box_data = fcst.to_dataframe(dim_order=[\"location\", \"ensemble\"])\n",
    "    box_data = box_data[fcst.name].unstack(level=0).to_numpy()\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 10))  # Create a new figure if no axis is provided\n",
    "\n",
    "    plt_box = ax.boxplot(\n",
    "        box_data,\n",
    "        showfliers=False,\n",
    "        vert=False,\n",
    "        labels=fcst.location.values,\n",
    "        patch_artist=True,\n",
    "        meanline=True,\n",
    "        showmeans=True,\n",
    "        meanprops=dict(linewidth=1, color=\"white\"),\n",
    "        medianprops=dict(linewidth=0, color=\"gray\", alpha=0),\n",
    "        whiskerprops=dict(linewidth=0.7, color=\"gray\"),\n",
    "        capprops=dict(linewidth=0.7, color=\"gray\"),\n",
    "        boxprops=dict(linewidth=0.7, color=\"gray\"),\n",
    "    )\n",
    "\n",
    "    [patch.set_facecolor(color) for patch, color in zip(plt_box[\"boxes\"], fcst.color.values)]\n",
    "    ax.set_xlabel(f\"{fcst.long_name} ({fcst.units})\")\n",
    "    ax.set_title(title)\n",
    "    legend_names = [\"japan\", \"alps\", \"pnw\", \"rockies\", \"new_england\"]\n",
    "    legend_handles = plt_box[\"boxes\"][: len(legend_names)]\n",
    "\n",
    "    if vline is not None:\n",
    "        ax.axvline(vline, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    if legend_loc == \"none\":\n",
    "        ax.set_yticklabels([])\n",
    "    else:\n",
    "        leg = ax.legend(legend_handles, legend_names, loc=legend_loc)\n",
    "        for patch, reg in zip(leg.get_patches(), legend_names):\n",
    "            patch.set_facecolor(colors[reg])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 15))\n",
    "plot_boxes(anom[\"swe\"], vline=100, legend_loc=\"upper left\", ax=ax1)\n",
    "plot_boxes(anom[\"precip\"], vline=100, legend_loc=\"none\", ax=ax2)\n",
    "plot_boxes(anomd[\"temp\"], vline=0, legend_loc=\"none\", ax=ax3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Variation at 4 Resorts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_names = [\"Whistler\", \"Kitzbuehl\", \"Copper\", \"Sugarloaf\"]\n",
    "focus_names = [\"RED\", \"Alpental\", \"Revelstoke\", \"Crystal Mtn\"]\n",
    "\n",
    "\n",
    "def plot_ensembles(fcst, clim, var=\"temp\", title=False):\n",
    "    \"\"\"Show ensemble values for a given variable.\"\"\"\n",
    "    x_val = \"season_day\"\n",
    "\n",
    "    favg = fcst.mean(dim=\"ensemble\", keep_attrs=True)\n",
    "    clr = fcst[\"color\"].values.tolist()\n",
    "\n",
    "    fcst[var].plot.line(x=x_val, color=\"grey\", alpha=0.1, add_legend=False)\n",
    "    favg[var].plot.line(x=x_val, color=\"black\", linewidth=2, add_legend=False)\n",
    "    clim[var].plot.line(x=x_val, color=clr, alpha=0.8, linewidth=2, add_legend=False)\n",
    "\n",
    "    plt.title(fcst[\"location\"].values if title else \"\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "\n",
    "(fig, axs) = plt.subplots(\n",
    "    nrows=3,\n",
    "    ncols=len(focus_names),\n",
    "    sharex=True,\n",
    "    sharey=\"row\",\n",
    "    figsize=(5 * len(focus_names), 15),\n",
    ")\n",
    "\n",
    "for idx in range(len(focus_names)):\n",
    "    fcst_loc = fcst.sel(location=focus_names[idx])\n",
    "    clim_loc = clim.sel(location=focus_names[idx])\n",
    "    plt.sca(axs[0, idx])\n",
    "    plot_ensembles(fcst_loc, clim_loc, \"swe\", title=True)\n",
    "    plt.sca(axs[1, idx])\n",
    "    plot_ensembles(fcst_loc, clim_loc, \"temp\")\n",
    "    plt.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "    plt.sca(axs[2, idx])\n",
    "    plot_ensembles(fcst_loc, clim_loc, \"precip\")\n",
    "    plt.gca().set_ylim((0, 40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('salient')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "51f8f0b1da789c29b6dfdfd0d04fb138f3b7f54f0cf7fdaf39807db9fc45e326"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
