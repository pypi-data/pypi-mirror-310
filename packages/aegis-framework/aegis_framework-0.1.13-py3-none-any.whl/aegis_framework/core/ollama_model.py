import subprocess

class OllamaLocalModel:
    """
    Example class to interface with a local Ollama model using CLI commands.
    """

    def __init__(self, model="default_model"):
        self.model = model

    def invoke(self, prompt):
        """
        Generate a response from the Ollama model using the CLI.

        Args:
            prompt (str): The input prompt for the model.

        Returns:
            str: The output generated by the model, or an error message if execution fails.
        """
        try:
            result = subprocess.run(
                ["ollama", "run", self.model],
                input=prompt,
                capture_output=True,
                text=True,
                check=True
            )
            return result.stdout.strip()
        except subprocess.CalledProcessError as e:
            return f"Error: {e.stderr}"
        except Exception as e:
            return f"Unexpected error: {e}"
